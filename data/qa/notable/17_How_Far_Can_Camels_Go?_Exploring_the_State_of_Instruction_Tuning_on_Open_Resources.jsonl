{"question": "What is the purpose of the paper?", "answer": "The purpose of the paper is to explore recent advances in instruction-tuning language models on a range of open instruction-following datasets and provide a comprehensive evaluation of these models."}
{"question": "What datasets were used to train the instruction-tuned models?", "answer": "The paper used 12 instruction datasets ranging from manually curated datasets like SuperNI and Open Assistant to generated datasets like Alpaca and Baize."}
{"question": "What model was used as the base language model for instruction tuning?", "answer": "The paper primarily used the LLAMA suite of pretrained models, ranging in size from 6.7B to 65B parameters, for instruction tuning."}
{"question": "What are the different evaluation metrics used in the paper?", "answer": "The paper used several evaluation metrics to assess the models, including factual knowledge (MMLU dataset), reasoning (GSM and BBH datasets), multilinguality (TyDiQA dataset), coding (Codex-Eval dataset), and open-ended instruction following (AlpacaFarm dataset and human evaluation)."}
{"question": "What was the best-performing model in the benchmark evaluations?", "answer": "The models trained on a mixture of human-authored and GPT-generated datasets (Human+GPT data mix) performed the best on average across all benchmark evaluations."}
{"question": "How does the performance of instruction-tuned models compare to GPT-4?", "answer": "The best instruction-tuned models reached on average 83% of ChatGPT performance and 68% of GPT-4 performance, suggesting that further improvement in base models and instruction-tuning data is required to close the performance gap."}
{"question": "Which dataset was helpful in improving mathematical reasoning in the GSM dataset?", "answer": "The CoT dataset was helpful in improving mathematical reasoning in the GSM dataset."}
{"question": "What is T\u00dcLU?", "answer": "T\u00dcLU is a suite of instruction-tuned models, ranging from 7B to 65B parameters, that are fine-tuned on a combination of high-quality open instruction datasets. T\u00dcLU performed the best on average across benchmark evaluations."}
