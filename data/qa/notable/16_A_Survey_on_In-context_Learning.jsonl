{"question": "What is the purpose of model warmup in in-context learning?", "answer": "Model warmup is an optional procedure that adjusts language models before in-context learning inference. It aims to enhance the overall in-context learning capability of the models, even though they have already shown promising in-context learning abilities."}
{"question": "What are the main strategies for demonstration designing in in-context learning?", "answer": "The main strategies for demonstration designing in in-context learning include demonstration organization and demonstration formatting. Demonstration organization focuses on selecting demonstration examples and determining their order. Demonstration formatting involves designing the format of demonstrations, including the use of instructions and intermediate reasoning steps."}
{"question": "What are the different ways to select demonstration examples in in-context learning?", "answer": "There are different ways to select demonstration examples in in-context learning. These include unsupervised methods based on pre-defined metrics, such as selecting the closest neighbors or using mutual information. Supervised methods utilize output scores of language models, such as selecting candidates based on retrieval scores or considering the influence of examples on model performance."}
{"question": "How can the order of demonstration examples impact the performance of in-context learning?", "answer": "The order of demonstration examples can have an impact on the performance of in-context learning. Some strategies focus on selecting the best ordering of examples based on metrics such as distance or entropy. The right ordering can improve the model's ability to learn from the demonstration context."}
{"question": "What are the different scoring functions used in in-context learning?", "answer": "There are different scoring functions used in in-context learning. These include direct estimation methods, such as estimating the conditional probability of candidate answers. Perplexity can also be used to evaluate the sentence probability of the input sequence. Channel models can estimate the conditional probability in the reversed direction, predicting the likelihood of the input query given the label."}
[{"title": "ReAct: Synergizing Reasoning and Acting in Language Models", "date": "6 Oct 2022", "url": "https://arxiv.org/pdf/2210.03629", "save_filepath": "data/qa/notable/7_ReAct:_Synergizing_Reasoning_and_Acting_in_Language_Models.jsonl", "out_title": "7_ReAct:_Synergizing_Reasoning_and_Acting_in_Language_Models.jsonl", "model": "gpt-3.5-turbo-16k", "paper_text": "Published as a conference paper at ICLR 2023\nREAC T: S YNERGIZING REASONING AND ACTING IN\nLANGUAGE MODELS\nShunyu Yao\u0003*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\n1Department of Computer Science, Princeton University\n2Google Research, Brain team\n1{shunyuy,karthikn}@princeton.edu\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\nABSTRACT\nWhile large language models (LLMs) have demonstrated impressive performance\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\nplan generation) have primarily been studied as separate topics. In this paper, we\nexplore the use of LLMs to generate both reasoning traces and task-speci\ufb01c actions\nin an interleaved manner, allowing for greater synergy between the two: reasoning\ntraces help the model induce, track, and update action plans as well as handle\nexceptions, while actions allow it to interface with and gather additional information\nfrom external sources such as knowledge bases or environments. We apply our\napproach, named ReAct , to a diverse set of language and decision making tasks\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\nimproved human interpretability and trustworthiness. Concretely, on question\nanswering (HotpotQA) and fact veri\ufb01cation (Fever), ReAct overcomes prevalent\nissues of hallucination and error propagation in chain-of-thought reasoning by\ninteracting with a simple Wikipedia API, and generating human-like task-solving\ntrajectories that are more interpretable than baselines without reasoning traces.\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\nan absolute success rate of 34% and 10% respectively, while being prompted with\nonly one or two in-context examples.\n1 I NTRODUCTION\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\nsider the example of cooking up a dish in the kitchen. Between any two speci\ufb01c actions, we may\nreason in language in order to track progress (\u201cnow that everything is cut, I should heat up the pot of\nwater\u201d), to handle exceptions or adjust the plan according to the situation (\u201cI don\u2019t have salt, so let\nme use soy sauce and pepper instead\u201d), and to realize when external information is needed (\u201chow do\nI prepare dough? Let me search on the Internet\u201d). We may also act (open a cookbook to read the\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (\u201cWhat\ndish can I make right now?\u201d). This tight synergy between \u201cacting\u201d and \u201creasoning\u201d allows humans\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\nunseen circumstances or facing information uncertainties.\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\n\u0003Work during Google internship. Projet page with code: https://react-lm.github.io/ .\n1arXiv:2210.03629v3  [cs.CL]  10 Mar 2023\n\nPublished as a conference paper at ICLR 2023\n$FW\u0003\u0014\u001d\u00037KLQN>)LUVW\u0003,\u0003QHHG\u0003WR\u0003ILQG\u0003D\u0003SHSSHU\u0003VKDNHU\u0011\u0011\u0011PRUH\u0003OLNHO\\\u0003WR\u0003DSSHDU\u0003LQ\u0003FDELQHWV\u0003\u000b\u0014\u0010\u0019\f\u000f\u0003FRXQWHUWRSV\u0003\u000b\u0014\u0010\u0016\f\u000f\u0003\u0011\u0011\u0011\u0003@$FW\u0003\u0015\u001d\u0003*R\u0003WR\u0003FDELQHW\u0003\u0014\u00032EV\u0003\u0015\u001d\u00032Q\u0003WKH\u0003FDELQHW\u0003\u0014\u000f\u0003\\RX\u0003VHH\u0003D\u0003YDVH\u0003\u0015\u0011\u0003\u000b+HUH\u0003WKH\u0003DJHQW\u0003JR\u0003WR\u0003FDELQHW\u0003\u0014\u000f\u0003WKHQ\u0003FDELQHW\u0003\u0015\u000f\u0003\u0016\u000f\u0003WKHQ\u0003FRXQWHUWRS\u0003\u0014\u0003DQG\u0003\u0015\f$FW\u0003\u0019\u001d\u0003*R\u0003WR\u0003FRXQWHUWRS\u0003\u0016\u00032EV\u0003\u0019\u001d\u00032Q\u0003WKH\u0003FRXQWHUWRS\u0003\u0016\u000f\u0003\\RX\u0003VHH\u0003D\u0003DSSOH\u0003\u0014\u000f\u0003D\u0003EUHDG\u0003\u0014\u000f\u0003\u0011\u0011\u0011\u0011\u0011\u0011\u0003D\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u000f\u0003DQG\u0003D\u0003YDVH\u0003\u0016\u0011$FW\u0003\u001a\u001d\u00037DNH\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0003IURP\u0003FRXQWHUWRS\u0003\u0016\u00032EV\u0003\u001a\u001d\u0003<RX\u0003SLFN\u0003XS\u0003WKH\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0003IURP\u0003WKH\u0003FRXQWHUWRS\u0003\u0016\u0011\u0003$FW\u0003\u001b\u001d\u00037KLQN>1RZ\u0003,\u0003ILQG\u0003D\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003WR\u0003SXW\u0003LW\u0003LQ\u0012RQ\u0003GUDZHU\u0003\u0014\u0011@$FW\u0003\u001c\u001d\u0003*R\u0003WR\u0003GUDZHU\u0003\u0014\u00032EV\u0003\u001c\u001d\u0003'UDZHU\u0003\u0014\u0003LV\u0003FORVHG\u0011\u0003$FW\u0003\u0014\u0013\u001d\u00032SHQ\u0003GUDZHU\u0003\u0014\u00032EV\u0003\u0014\u0013\u001d\u0003<RX\u0003RSHQ\u0003'UDZHU\u0003\u0014\u0003\u00aa$FW\u0003\u0014\u0014\u001d\u00033XW\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0003LQ\u0012RQ\u0003GUDZHU\u0003\u0014\u00032EV\u0003\u0014\u0014\u001d\u0003<RX\u0003SXW\u0003SHSSHU\u0003VKDNHU\u0003\u0014\u0003LQ\u0012RQ\u0003WKH\u0003GUDZHU\u0003\u0014\u0011\u000b\u0014D\f\u00036WDQGDUG$QVZHU\u001d\u0003L3RG\u000b\u0014E\f\u0003&R7\u0003\u000b5HDVRQ\u00032QO\\\f7KRXJKW\u001d\u0003/HW\nV\u0003WKLQN\u0003VWHS\u0003E\\\u0003VWHS\u0011\u0003$SSOH\u00035HPRWH\u0003ZDV\u0003RULJLQDOO\\\u0003GHVLJQHG\u0003WR\u0003LQWHUDFW\u0003ZLWK\u0003$SSOH\u000379\u0011\u0003$SSOH\u000379\u0003FDQ\u0003EH\u0003FRQWUROOHG\u0003E\\\u0003L3KRQH\u000f\u0003L3DG\u000f\u0003DQG\u0003L3RG\u00037RXFK\u0011\u00036R\u0003WKH\u0003DQVZHU\u0003LV\u0003L3KRQH\u000f\u0003L3DG\u000f\u0003DQG\u0003L3RG\u00037RXFK\u0011$QVZHU\u001d\u0003L3KRQH\u000f\u0003L3DG\u000f\u0003L3RG\u00037RXFK\u000b\u0014F\f\u0003$FW\u00102QO\\\u0003$FW\u0003\u0014\u001d\u00036HDUFK>$SSOH\u00035HPRWH@\u00032EV\u0003\u0014\u001d\u00037KH\u0003$SSOH\u00035HPRWH\u0003LV\u0003D\u0003UHPRWH\u0003FRQWURO\u0003\u00aa$FW\u0003\u0015\u001d\u00036HDUFK>)URQW\u00035RZ@\u00032EV\u0003\u0015\u001d\u0003&RXOG\u0003QRW\u0003ILQG\u0003>)URQW\u00035RZ@\u0011\u00036LPLODU\u001d\u0003\u00aa$FW\u0003\u0016\u001d\u00036HDUFK>)URQW\u00035RZ\u0003\u000bVRIWZDUH\f@\u00032EV\u0003\u0016\u001d\u0003)URQW\u00035RZ\u0003LV\u0003D\u0003GLVFRQWLQXHG\u0003PHGLD\u0003FHQWHU\u0003VRIWZDUH\u0003\u00aa$FW\u0003\u0017\u001d\u0003)LQLVK>\\HV@\u000b\u0014G\f\u00035H$FW\u0003\u000b5HDVRQ\u0003\u000e\u0003$FW\f7KRXJKW\u0003\u0014\u001d\u0003,\u0003QHHG\u0003WR\u0003VHDUFK\u0003$SSOH\u00035HPRWH\u0003DQG\u0003ILQG\u0003WKH\u0003SURJUDP\u0003LW\u0003ZDV\u0003RULJLQDOO\\\u0003GHVLJQHG\u0003WR\u0003LQWHUDFW\u0003ZLWK\u0011$FW\u0003\u0014\u001d\u00036HDUFK>$SSOH\u00035HPRWH@2EV\u0003\u0014\u001d\u00037KH\u0003$SSOH\u00035HPRWH\u0003LV\u0003D\u0003UHPRWH\u0003FRQWURO\u0003LQWURGXFHG\u0003LQ\u00032FWREHU\u0003\u0015\u0013\u0013\u0018\u0003E\\\u0003$SSOH\u0003\u00aa\u0003RULJLQDOO\\\u0003GHVLJQHG\u0003WR\u0003FRQWURO\u0003WKH\u0003)URQW\u00035RZ\u0003PHGLD\u0003FHQWHU\u0003SURJUDP\u0003\u00aa7KRXJKW\u0003\u0015\u001d\u0003$SSOH\u00035HPRWH\u0003ZDV\u0003RULJLQDOO\\\u0003GHVLJQHG\u0003WR\u0003FRQWURO\u0003WKH\u0003)URQW\u00035RZ\u0003PHGLD\u0003FHQWHU\u0003SURJUDP\u0011\u0003,\u0003QHHG\u0003WR\u0003VHDUFK\u0003)URQW\u00035RZ\u0003QH[W\u0003DQG\u0003ILQG\u0003ZKDW\u0003RWKHU\u0003GHYLFH\u0003FDQ\u0003FRQWURO\u0003LW\u0011$FW\u0003\u0015\u001d\u00036HDUFK>)URQW\u00035RZ@2EV\u0003\u0015\u001d\u0003&RXOG\u0003QRW\u0003ILQG\u0003>)URQW\u00035RZ@\u0011\u00036LPLODU\u001d\u0003>\n)URQW\u00035RZ\u00036HDW\u0003WR\u0003(DUWK\n\u000f\u0003\n)URQW\u00035RZ\u00030RWRUVSRUWV\n\u000f\n)URQW\u00035RZ\u0003\u000bVRIWZDUH\f\n\u000f\u0003\u00aa@7KRXJKW\u0003\u0016\u001d\u0003)URQW\u00035RZ\u0003LV\u0003QRW\u0003IRXQG\u0011\u0003,\u0003QHHG\u0003WR\u0003VHDUFK\u0003)URQW\u00035RZ\u0003\u000bVRIWZDUH\f\u0003\u0011$FW\u0003\u0016\u001d\u00036HDUFK>)URQW\u00035RZ\u0003\u000bVRIWZDUH\f@2EV\u0003\u0016\u001d\u0003)URQW\u00035RZ\u0003LV\u0003D\u0003GLVFRQWLQXHG\u0003PHGLD\u0003FHQWHU\u0003VRIWZDUH\u0003\u00aa7KRXJKW\u0003\u0017\u001d\u0003)URQW\u00035RZ\u0003\u000bVRIWZDUH\f\u0003LV\u0003FRQWUROOHG\u0003E\\\u0003DQ\u0003$SSOH\u00035HPRWH\u0003RU\u0003WKH\u0003NH\\ERDUG\u0003IXQFWLRQ\u0003NH\\V\u0011\u00036R\u0003WKH\u0003DQVZHU\u0003LV\u0003NH\\ERDUG\u0003IXQFWLRQ\u0003NH\\V\u0011$FW\u0003\u0017\u001d\u0003)LQLVK>NH\\ERDUG\u0003IXQFWLRQ\u0003NH\\V@\u04bc4XHVWLRQ\u001d\u0003$VLGH\u0003IURP\u0003WKH\u0003$SSOH\u00035HPRWH\u000f\u0003ZKDW\u0003RWKHU\u0003GHYLFH\u0003FDQ\u0003FRQWURO\u0003WKH\u0003SURJUDP\u0003$SSOH\u00035HPRWH\u0003ZDV\u0003RULJLQDOO\\\u0003GHVLJQHG\u0003WR\u0003LQWHUDFW\u0003ZLWK\"\n\u000b\u0015D\f\u0003$FW\u00102QO\\\u0003$FW\u0003\u0014\u001d\u0003*R\u0003WR\u0003GUDZHU\u0003\u0014\u00032EV\u0003\u0014\u001d\u00037KH\u0003GUDZHU\u0003\u0014\u0003LV\u0003FORVHG\u0011\u0003$FW\u0003\u0015\u001d\u00032SHQ\u0003GUDZHU\u0003\u0014\u00032EV\u0003\u0015\u001d\u0003<RX\u0003RSHQ\u0003WKH\u0003GUDZHU\u0003\u0014\u0011\u00037KH\u0003GUDZHU\u0003\u0014\u0003LV\u0003RSHQ\u0011\u0003,Q\u0003LW\u000f\u0003\\RX\u0003VHH\u0003D\u0003GLVKVSRQJH\u0003\u0015\u000f\u0003DQG\u0003D\u0003VSRRQ\u0003\u0014\u0011\u0003$FW\u0003\u0016\u001d\u0003*R\u0003WR\u0003VLQNEDVLQ\u0003\u0014\u00032EV\u0003\u0016\u001d\u00032Q\u0003WKH\u0003VLQNEDVLQ\u0003\u0014\u000f\u0003\\RX\u0003VHH\u0003D\u0003GLVKVSRQJH\u0003\u0016\u000f\u0003D\u0003VSDWXOD\u0003\u0014\u000f\u0003DQG\u0003D\u0003VSRRQ\u0003\u0015\u0011\u0003$FW\u0003\u0017\u001d\u00037DNH\u0003SHSSHUVKDNHU\u0003\u0014\u0003IURP\u0003VLQNEDVLQ\u0003\u0014\u00032EV\u0003\u0017\u001d\u00031RWKLQJ\u0003KDSSHQV\u0011\u0003$FW\u0003\u0018\u001d\u00037DNH\u0003SHSSHUVKDNHU\u0003\u0014\u0003IURP\u0003VLQNEDVLQ\u0003\u0014\u00032EV\u0003\u0018\u001d\u00031RWKLQJ\u0003KDSSHQV\u0011\u0003\u000b\u0015E\f\u00035H$FW\u0003\u000b5HDVRQ\u0003\u000e\u0003$FW\f\n\u04bc<RX\u0003DUH\u0003LQ\u0003WKH\u0003PLGGOH\u0003RI\u0003D\u0003URRP\u0011\u0003/RRNLQJ\u0003TXLFNO\\\u0003DURXQG\u0003\\RX\u000f\u0003\\RX\u0003VHH\u0003D\u0003FDELQHW\u0003\u0019\u000f\u0003D\u0003FDELQHW\u0003\u0014\u000f\u0003D\u0003FRIIHH\u0003PDFKLQH\u0003\u0014\u000f\u0003D\u0003FRXQWHUWRS\u0003\u0016\u000f\u0003\u0003D\u0003VWRYH\u0003EXUQHU\u0003\u0014\u000f\u0003DQG\u0003D\u0003WRDVWHU\u0003\u0014\u0011\u0003<RXU\u0003WDVN\u0003LV\u0003WR\u001d\u00033XW\u0003VRPH\u0003SHSSHU\u0003VKDNHU\u0003RQ\u0003D\u0003GUDZHU\u0011\u000b\u0015\f\u0003$OI:RUOG\u000b\u0014\f\u0003+RWVSRW\u00034$\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) Chain-of-thought ( CoT,\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\n2022). However, this \u201cchain-of-thought\u201d reasoning is a static black box, in that the model uses\nits own internal representations to generate thoughts and is not grounded in the external world,\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,\nrecent work has explored the use of pre-trained language models for planning and acting in interactive\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\na focus on predicting actions via language priors. These approaches usually convert multi-modal\nobservations into text, use a language model to generate domain-speci\ufb01c actions or plans, and then\nuse a controller to choose or execute them. However, they do not employ language models to reason\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\nand if such a combination can bring systematic bene\ufb01ts compared to reasoning or acting alone.\nIn this work, we present ReAct , a general paradigm to combine reasoning and acting with language\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\nadjust high-level plans for acting (reason to act), while also interact with the external environments\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\n2\n\nPublished as a conference paper at ICLR 2023\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\nquestion answering (HotPotQA, Yang et al., 2018), fact veri\ufb01cation (Fever, Thorne et al., 2018),\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-\nthought reasoning ( CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\nandCoT that allows for the use of both internal knowledge and externally obtained information\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\nto outperform imitation or reinforcement learning methods trained with 103\u0018105task instances,\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\nthe importance of sparse, versatile reasoning in decision making by showing consistent advantages\nover controlled baselines with actions only. Besides general applicability and performance boost,\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\nand diagnosability across all domains, as humans can readily distinguish information from model\u2019s\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\nthe decision basis of model actions.\nTo summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt-\nbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\nperform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in\nisolation; (3) we present systematic ablations and analysis to understand the importance of acting in\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial \ufb01netuning\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\nReAct to train and operate on more tasks and combining it with complementary paradigms like\nreinforcement learning could further unlock the potential of large language models.\n2REAC T: SYNERGIZING REASONING +AC TING\nConsider a general setup of an agent interacting with an environment for task solving. At time\nstept, an agent receives an observation ot2O from the environment and takes an action at2A\nfollowing some policy \u0019(atjct), wherect= (o1;a1;\u0001\u0001\u0001;ot\u00001;at\u00001;ot)is the context to the agent.\nLearning a policy is challenging when the mapping ct7!atis highly implicit and requires extensive\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct \ufb01nal\naction (Act 4) to \ufb01nish the QA task as it requires complex reasoning over the trajectory context\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\nThe idea of ReAct is simple: we augment the agent\u2019s action space to ^A=A[L , whereLis the\nspace of language. An action ^at2L in the language space, which we will refer to as a thought or a\nreasoning trace , does not affect the external environment, thus leading to no observation feedback.\nInstead, a thought ^ataims to compose useful information by reasoning over the current context ct,\nand update the context ct+1= (ct;^at)to support future reasoning or acting. As shown in Figure 1,\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans\n(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\nHowever, as the language space Lis unlimited, learning in this augmented action space is dif\ufb01cult\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\nlarge language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context\nexamples to generate both domain-speci\ufb01c actions and free-form language thoughts for task solving\n(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and\nenvironment observations to solve a task instance (see Appendix C). For the tasks where reasoning is\nof primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the\ntask-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision\nmaking tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to\n1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.\n3\n\nPublished as a conference paper at ICLR 2023\nappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\nasynchronous occurrence of thoughts and actions for itself.\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\nenjoys several unique features: A) Intuitive and easy to design : Designing ReAct prompts is\nstraightforward as human annotators just type down their thoughts in language on top of their actions\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\nprompt design for each task in Sections 3 and 4. B) General and \ufb02exible : Due to the \ufb02exible thought\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\nspaces and reasoning needs, including but not limited to QA, fact veri\ufb01cation, text game, and web\nnavigation. C) Performant and robust :ReAct shows strong generalization to new task instances\nwhile learning solely from one to six in-context examples, consistently outperforming baselines with\nonly reasoning or acting across different domains. We also show in Section 3 additional bene\ufb01ts\nwhen \ufb01netuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\nD) Human aligned and controllable :ReAct promises an interpretable sequential decision making\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\nFigure 5 in Section 4.\n3 K NOWLEDGE -INTENSIVE REASONING TASKS\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\nveri\ufb01cation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\ndemonstrating a synergy of reasoning and acting.\n3.1 S ETUP\nDomains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\nPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\nover two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veri\ufb01cation\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\nsetup for both tasks, where models only receive the question/claim as input without access to support\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\nan external environment to support reasoning.\nAction Space We design a simple Wikipedia web API with three types of actions to support\ninteractive information retrieval: (1) search [entity ], which returns the \ufb01rst 5 sentences from\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\nWikipedia search engine, (2) lookup [string ], which would return the next sentence in the page\ncontaining string , simulating Ctrl+F functionality on the browser. (3) finish [answer ], which\nwould \ufb01nish the current task with answer . We note that this action space mostly can only retrieve a\nsmall part of a passage based on exact passage name, which is signi\ufb01cantly weaker than state-of-the-\nart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\nand force models to retrieve via explicit reasoning in language.\n3.2 M ETHODS\nReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2from the training\nset and manually compose ReAct -format trajectories to use as few-shot exemplars in the prompts.\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\nthought), where free-form thoughts are used for various purposes. Speci\ufb01cally, we use a combination\nof thoughts that decompose questions (\u201cI need to search x, \ufb01nd y, then \ufb01nd z\u201d), extract information\nfrom Wikipedia observations (\u201cx was started in 1844\u201d, \u201cThe paragraph does not tell x\u201d), perform\ncommonsense (\u201cx is not y, so z must instead be...\u201d) or arithmetic reasoning (\u201c1844 < 1989\u201d), guide\n2We \ufb01nd more examples do not improve performance.\n4\n\nPublished as a conference paper at ICLR 2023\nPrompt Methoda HotpotQA Fever\n(EM) (Acc)\nStandard 28.7 57.1\nCoT (Wei et al., 2022) 29.4 56.3\nCoT-SC (Wang et al., 2022a) 33.4 60.4\nAct 25.7 58.9\nReAct 27.4 60.9\nCoT-SC!ReAct 34.2 64.6\nReAct!CoT-SC 35.1 62.0\nSupervised SoTAb67.5 89.5\nTable 1: PaLM-540B prompting results on\nHotpotQA and Fever.\naHotpotQA EM is 27.1, 28.9, 33.8 for Standard ,CoT,\nCoT-SC in Wang et al. (2022b).\nb(Zhu et al., 2021; Lewis et al., 2020)\n0 5 10 15 20\n#CoT-SC trials2628303234HotpotQA EM\n0 5 10 15 20\n#CoT-SC trials47.550.052.555.057.560.062.565.0Fever AccMethod\nCoT-SC -> ReAct\nReAct -> CoT-SC\nCoT-SC\nReAct\nCoTFigure 2: PaLM-540B prompting results with respect to\nnumber of CoT-SC samples used.\nsearch reformulation (\u201cmaybe I can search/look up x instead\u201d), and synthesize the \ufb01nal answer (\u201c...so\nthe answer is x\u201d). See Appendix C for more details.\nBaselines We systematically ablate ReAct trajectories to build prompts for multiple baselines (with\nformats as Figure 1(1a-1c)): (a) Standard prompting (Standard ), which removes all thoughts,\nactions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,\n2022), which removes actions and observations and serve as a reasoning-only baseline. We also\nbuild a self-consistency baseline ( CoT-SC ) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\nconsistently boost performance over CoT. (c)Acting-only prompt (Act), which removes thoughts\ninReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\nand reinforcement learning instead of prompting.\nCombining Internal and External Knowledge As will be detail in Section 3.3, we observe that\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\nor thoughts. We therefore propose to incorporate ReAct andCoT-SC , and let the model decide\nwhen to switch to the other method based on the following heuristics: A) ReAct!CoT-SC : when\nReAct fails to return an answer within given steps, back off to CoT-SC . We set 7 and 5 steps for\nHotpotQA and FEVER respectively as we \ufb01nd more steps will not improve ReAct performance3.\nB)CoT-SC!ReAct : when the majority answer among nCoT-SC samples occurs less than n=2\ntimes (i.e. internal knowledge might not support the task con\ufb01dently), back off to ReAct .\nFinetuning Due to the challenge of manually annotating reasoning traces and actions at scale,\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\nwith correct answers generated by ReAct (also for other baselines) to \ufb01netune smaller language\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\ninput questions/claims. More details are in Appendix B.1.\n3.3 R ESULTS AND OBSERVATIONS\nReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-\n540B as the base model with different prompting methods. We note that ReAct is better than Act\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\n\ufb01nal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also con\ufb01rm the bene\ufb01t of reasoning\ntraces for more informed acting.\n3Of all trajectories with correct \ufb01nal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\ntake up 0.84% and 1.33% respectively.\n5\n\nPublished as a conference paper at ICLR 2023\nType De\ufb01nition ReAct CoT\nSuccessTrue positive Correct reasoning trace and facts 94% 86%\nFalse positive Hallucinated reasoning trace or facts 6% 14%\nFailureReasoning error Wrong reasoning trace (including failing to recover from repetitive steps) 47% 16%\nSearch result error Search return empty or does not contain useful information 23% -\nHallucination Hallucinated reasoning trace or facts 0% 56%\nLabel ambiguity Right prediction but did not match the label precisely 29% 28%\nTable 2: Types of success and failure modes of ReAct andCoT on HotpotQA, as well as their\npercentages in randomly selected examples studied by human.\nReAct vs.CoT On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\nlags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\ndiffer by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\nis vital. To better understand the behavioral difference between ReAct andCoT on HotpotQA, we\nrandomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\nandCoT respectively (thus 200 examples in total), and manually labeled their success and failure\nmodes in Table 2. Some key observations are as follows:\nA)Hallucination is a serious problem for CoT, resulting in much higher false positive rate than\nReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\nproblem solving trajectory of ReAct is more grounded, fact-driven, and trustworthy, thanks to the\naccess of an external knowledge base.\nB)While interleaving reasoning, action and observation steps improves ReAct \u2019s grounded-\nness and trustworthiness, such a structural constraint also reduces its \ufb02exibility in formulating\nreasoning steps , leading to more reasoning error rate than CoT. we note that there is one frequent\nerror pattern speci\ufb01c to ReAct , in which the model repetitively generates the previous thoughts and\nactions, and we categorize it as part of \u201creasoning error\u201d as the model fails to reason about what the\nproper next action to take and jump out of the loop4.\nC)ForReAct , successfully retrieving informative knowledge via search is critical. Non-\ninformative search, which counts for 23% of the error cases, derails the model reasoning and gives\nit a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between\nfactuality and \ufb02exibility, which motivates our proposed strategies of combining two methods.\nWe provide examples for each success and failure modes in Appendix E.1. We also \ufb01nd some\nHotpotQA questions may contain outdated answer labels, see Figure 4 for example.\nReAct +CoT-SC perform best for prompting LLMs Also shown in Table 1, the best prompting\nmethod on HotpotQA and Fever are ReAct!CoT-SC andCoT-SC!ReAct respectively.\nFurthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC\nsamples used. While two ReAct +CoT-SC methods are advantageous at one task each, they both\nsigni\ufb01cantly and consistently outperform CoT-SC across different number of samples, reaching\nCoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of\nproperly combining model internal knowledge and external knowledge for reasoning tasks.\nReAct performs best for \ufb01ne-tuning Figure 3 shows the scaling effect of prompting/\ufb01netuning\nfour methods ( Standard ,CoT,Act,ReAct ) on HotpotQA. With PaLM-8/62B, prompting ReAct\nperforms worst among four methods due to the dif\ufb01culty to learn both reasoning and acting from\nin-context examples. However, when \ufb01netuned with just 3,000 examples, ReAct becomes the best\nmethod among the four, with PaLM-8B \ufb01netuned ReAct outperforming all PaLM-62B prompting\nmethods, and PaLM-62B \ufb01netuned ReAct outperforming all 540B prompting methods. In contrast,\n\ufb01netuning Standard orCoT is signi\ufb01cantly worse than \ufb01netuning ReAct orAct for both PaLM-\n8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge\nfacts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\nmore generalizable skill for knowledge reasoning. As all prompting methods are still signi\ufb01cantly\nfar from domain-speci\ufb01c state-of-the-art approaches (Table 1), we believe \ufb01netuning with more\nhuman-written data might be a better way to unleash the power of ReAct .\n4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using\nbetter decoding (e.g. beam search) might help address this issue.\n6"}, ["string indices must be integers", "string indices must be integers", "string indices must be integers"]]
