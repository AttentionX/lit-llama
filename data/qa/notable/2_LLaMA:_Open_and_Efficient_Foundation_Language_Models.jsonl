{"question": "What is LLaMA?", "answer": "LLaMA is a collection of foundation language models ranging from 7B to 65B parameters. It is trained on trillions of tokens using publicly available datasets only."}
{"question": "What is the main contribution of LLaMA?", "answer": "The main contribution of LLaMA is the ability to train state-of-the-art language models using publicly available datasets exclusively, without relying on proprietary or inaccessible data. LLaMA-13B outperforms GPT-3 on most benchmarks, and LLaMA-65B is competitive with the best existing models."}
{"question": "What are the benchmarks used to evaluate LLaMA?", "answer": "LLaMA is evaluated on various benchmarks including common sense reasoning tasks (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA), closed-book question answering (Natural Questions, TriviaQA), reading comprehension (RACE), mathematical reasoning (MATH, GSM8k), code generation (HumanEval, MBPP), and massive multitask language understanding (MMLU)."}
{"question": "How does LLaMA compare to other models on the common sense reasoning tasks?", "answer": "LLaMA-65B outperforms Chinchilla-70B on all common sense reasoning benchmarks except BoolQ. It also surpasses PaLM-540B on most benchmarks. LLaMA-13B outperforms GPT-3 on most benchmarks despite being smaller in size."}
{"question": "How does LLaMA perform on closed-book question answering?", "answer": "LLaMA-65B achieves state-of-the-art performance on both Natural Questions and TriviaQA benchmarks in the zero-shot and few-shot settings. LLaMA-13B is also competitive with GPT-3 and Chinchilla on these benchmarks."}
{"question": "How does LLaMA perform on reading comprehension?", "answer": "LLaMA-65B is competitive with PaLM-540B on the RACE reading comprehension benchmark. LLaMA-13B outperforms GPT-3 on this benchmark by a few percentage points."}
{"question": "How does LLaMA perform on mathematical reasoning?", "answer": "LLaMA-65B outperforms Minerva-62B on the GSM8k mathematical reasoning dataset, despite not being fine-tuned on mathematical data. LLaMA-13B also achieves competitive performance on these benchmarks."}
{"question": "How does LLaMA perform on code generation?", "answer": "LLaMA-65B outperforms PaLM-62B on both the HumanEval and MBPP code generation benchmarks, despite not being fine-tuned for code. LLaMA-13B also achieves strong performance on these tasks, outperforming other models such as LaMDA."}
{"question": "How does LLaMA perform on the massive multitask language understanding benchmark?", "answer": "LLaMA-65B is slightly behind Chinchilla-70B and PaLM-540B in terms of average performance on the MMLU benchmark. A potential reason is the relatively limited amount of book data used in pre-training compared to the other models."}
{"question": "How does LLaMA's performance evolve during training?", "answer": "LLaMA shows steady improvement in performance on most benchmarks during training, with performance correlating with the training perplexity. However, there is some variance in performance on certain benchmarks such as SIQA and WinoGrande."}
