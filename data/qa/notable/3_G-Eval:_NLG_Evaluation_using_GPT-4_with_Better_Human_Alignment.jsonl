{"question": "What is the main contribution of G-E VAL?", "answer": "The main contribution of G-E VAL is the framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm to assess the quality of NLG outputs. It achieves high correlation with human judgments in NLG evaluation tasks, outperforming all previous methods."}
{"question": "What are the two generation tasks that G-E VAL experiments with?", "answer": "G-E VAL experiments with two generation tasks: text summarization and dialogue generation."}
{"question": "What are the limitations of conventional reference-based metrics for NLG evaluation?", "answer": "Conventional reference-based metrics, such as BLEU and ROUGE, have relatively low correlation with human judgments, especially for tasks that require creativity and diversity in text generation."}
{"question": "What is the concern regarding LLM-based evaluators?", "answer": "The concern regarding LLM-based evaluators is that they may have a bias towards the LLM-generated texts, which could lead to the self-reinforcement of LLMs if used as the reward signal for improving themselves."}
{"question": "What is the role of chain-of-thoughts (CoT) in G-E VAL?", "answer": "Chain-of-thoughts (CoT) in G-E VAL provides more context and guidance for the LLM to evaluate the generated text, improving the performance of the evaluator."}
{"question": "What is the significance of using large language models (LLMs) as reference-free metrics for NLG evaluation?", "answer": "Using LLMs as reference-free metrics allows for evaluation on new tasks that lack human references, providing a more applicable approach for evaluating NLG systems."}
{"question": "What is the scoring function used in G-E VAL?", "answer": "The scoring function in G-E VAL calls the LLM with the designed prompt, auto chain-of-thoughts (CoT), the input context, and the target text to calculate the score based on the probabilities of the return tokens."}
{"question": "What are the three benchmarks used to measure the correlation between G-E VAL and human judgments?", "answer": "The three benchmarks used are SummEval, Topical-Chat, and QAGS, which evaluate different aspects of NLG tasks including summarization and dialogue generation."}
{"question": "What is the G-E VAL's correlation score on the SummEval benchmark with GPT-4 as the backbone model?", "answer": "G-E VAL with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on the summarization task, outperforming all previous methods by a large margin."}
{"question": "What is the approach used by G-E VAL to evaluate text summarization?", "answer": "G-E VAL uses a form-filling paradigm, providing a prompt along with a generated chain-of-thoughts (CoT) to evaluate the coherence, consistency, fluency, and relevance of text summarization outputs."}
