{"question": "What is LLaMA?", "answer": "LLaMA is a collection of foundation language models ranging from 7B to 65B parameters that were trained on trillions of tokens. It achieves state-of-the-art performance on various benchmarks and can be run on a single GPU."}
{"question": "How does LLaMA compare to GPT-3?", "answer": "LLaMA-13B outperforms GPT-3 (175B) on most benchmarks and LLaMA-65B is competitive with other large language models like Chinchilla-70B and PaLM-540B."}
{"question": "What datasets were used to train LLaMA?", "answer": "The training dataset for LLaMA is a mixture of several publicly available sources, including English CommonCrawl, C4, Github, Wikipedia, Gutenberg, Books3, ArXiv, and Stack Exchange."}
{"question": "What modifications were made to the transformer architecture?", "answer": "LLaMA uses a pre-normalization technique, a SwiGLU activation function, and rotary embeddings instead of absolute positional embeddings in the transformer architecture."}
{"question": "How efficient is LLaMA's implementation?", "answer": "When training a 65B-parameter model, LLaMA processes around 380 tokens/sec/GPU on a single V100 GPU with 80GB of RAM."}
{"question": "In which domains does LLaMA achieve state-of-the-art performance?", "answer": "LLaMA achieves state-of-the-art performance in common sense reasoning, closed-book question answering, reading comprehension, mathematical reasoning, and code generation."}
{"question": "How does LLaMA perform on the MMLU benchmark?", "answer": "LLaMA-65B is slightly behind Chinchilla-70B and PaLM-540B in average performance on the MMLU benchmark in the 5-shot setting."}
{"question": "What is the training approach for LLaMA?", "answer": "LLaMA is trained using publicly available datasets, without resorting to proprietary and inaccessible datasets. The training approach is similar to previous work, using a large quantity of textual data and a standard optimizer like AdamW."}
{"question": "How does the performance of LLaMA evolve during training?", "answer": "On most benchmarks, the performance of LLaMA steadily improves during training and correlates with the training perplexity of the model."}
