{"question": "What is G-E VAL?", "answer": "G-E VAL is a framework that uses large language models with chain-of-thoughts and a form-filling paradigm to evaluate the quality of natural language generation (NLG) outputs. It achieves high correlations with human judgments on tasks like summarization and dialogue generation."}
{"question": "What are some conventional reference-based metrics for NLG evaluation?", "answer": "Some conventional reference-based metrics for NLG evaluation are BLEU, ROUGE, and METEOR."}
{"question": "What are LLM-based evaluators?", "answer": "LLM-based evaluators are reference-free NLG evaluators that use large language models to score the candidate output based on its generation probability without any reference target."}
{"question": "What is G-E VAL's scoring function?", "answer": "G-E VAL's scoring function calls the LLM and calculates the score based on the probabilities of the return tokens. The final score is obtained by taking the weighted summation of the output scores."}
{"question": "How does G-E VAL perform compared to other evaluators on the SummEval benchmark?", "answer": "G-E VAL achieves higher correlations with human judgments compared to other evaluators on the SummEval benchmark, especially G-E VAL-4 which outperforms all previous methods by a large margin."}
{"question": "What is the purpose of chain-of-thoughts (CoT) in G-E VAL?", "answer": "Chain-of-thoughts (CoT) provides more context and guidance for the LLM to evaluate the generated text and helps to explain the evaluation process and results."}
{"question": "What is the concern about using LLM-based evaluators?", "answer": "One concern is that LLM-based evaluators may prefer the outputs generated by the LLM itself, rather than high-quality human-written texts, leading to a bias towards LLM-generated text."}
{"question": "How can the behavior of LLM-based evaluators be addressed?", "answer": "Further research is needed to fully understand the behavior of LLM-based evaluators and reduce their inherent bias towards LLM-generated text, to avoid overfitting to LLM's own evaluation criteria."}
{"question": "What is the effect of chain-of-thoughts in G-E VAL on the SummEval benchmark?", "answer": "G-E VAL with chain-of-thoughts (CoT) has higher correlations with human judgments compared to G-E VAL without CoT, especially for the dimension of fluency."}
{"question": "What is the effect of probability normalization in G-E VAL on the SummEval benchmark?", "answer": "On the Kendall-Tau correlation, G-E VAL with probability normalization achieves higher correlations compared to G-E VAL without probability normalization."}
