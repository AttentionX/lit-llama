title,published,modified,pdfUrl,conference
LLaMA: Open and Efficient Foundation Language Models,27 Feb 2023,27 Feb 2023,https://arxiv.org/pdf/2302.13971
G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment,29 Mar 2023,23 May 2023,https://arxiv.org/abs/2303.16634
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,30 Mar 2023,25 May 2023,https://arxiv.org/pdf/2303.17580
Generative Agents: Interactive Simulacra of Human Behavior,7 Apr 2023,7 Apr 2023https://arxiv.org/pdf/2304.03442
LIMA: Less Is More for Alignment,18 May 2023,18 May 2023,https://arxiv.org/pdf/2305.11206
ReAct: Synergizing Reasoning and Acting in Language Models,6 Oct 2022,10 Mar 2023,https://arxiv.org/pdf/2210.03629
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,28 Mar 2023,28 Mar 2023,https://arxiv.org/pdf/2303.16199
LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model,28 Apr 2023,28 Apr 2023,https://arxiv.org/pdf/2304.15010
GPT-4 Technical Report,15 Mar 2023,27 Mar 2023,https://arxiv.org/pdf/2303.08774
Gorilla: Large Language Model Connected with Massive APIs,24 May 2023,24 May 2023,https://arxiv.org/pdf/2305.15334
Constitutional AI: Harmlessness from AI Feedback,15 Dec 2022,15 Dec 2022,https://arxiv.org/pdf/2212.08073
Training language models to follow instructions with human feedback,4 Mar 2022,4 Mar 2022,https://arxiv.org/pdf/2203.02155
UL2: Unifying Language Learning Paradigms,10 May 2022,28 Feb 2023,https://arxiv.org/pdf/2205.05131
CLIPPO: Image-and-Language Understanding from Pixels Only,15 Dec 2022,1 Apr 2023,https://arxiv.org/pdf/2212.08045
Structured Prompting: Scaling In-Context Learning to 1,000 Examples,13 Dec 2022,13 Dec 2022,https://arxiv.org/pdf/2212.06713
A Length-Extrapolatable Transformer,20 Dec 2022,20 Dec 2022,https://arxiv.org/pdf/2212.10554
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,28 Jan 2022,10 Jan 2023,https://arxiv.org/pdf/2201.11903
Training Compute-Optimal Large Language Models,29 Mar 2022,29 Mar 2022,https://arxiv.org/pdf/2203.15556
A Generalist Agent,12 May 2022,11 Nov 2022,https://arxiv.org/pdf/2205.06175,TMLR
Large Language Models are Zero-Shot Reasoners,24 May 2022,29 Jan 2023,https://arxiv.org/pdf/2205.11916,NeurIPS 2022
Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts,6 Jun 2022,6 Jun 2022,https://arxiv.org/pdf/2206.02770
Emergent Abilities of Large Language Models,01 Sept 2022,01 Mar 2023,https://openreview.net/pdf?id=yzkSU5zdwD,TMLR 2023
Scaling Instruction-Finetuned Language Models,20 Oct 2022,6 Dec 2022,https://arxiv.org/pdf/2210.11416
Transcending Scaling Laws with 0.1% Extra Compute,20 Oct 2022,6 Nov 2022,https://arxiv.org/pdf/2210.11399
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning,13 Jun 2023,13 Jun 2023,https://arxiv.org/pdf/2306.07967
WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences,13 Jun 2023,13 Jun 2023,https://arxiv.org/pdf/2306.07906,KDD 2023
TART: A plug-and-play Transformer module for task-agnostic reasoning,13 Jun 2023,13 Jun 2023,https://arxiv.org/pdf/2306.07536
ATT3D: Amortized Text-to-3D Object Synthesis,6 Jun 2023,6 Jun 2023,https://arxiv.org/pdf/2306.07349
Benchmarking Neural Network Training Algorithms,12 Jun 2023,12 Jun 2023,https://arxiv.org/pdf/2306.07179
Augmenting Language Models with Long-Term Memory,12 Jun 2023,12 Jun 2023,https://arxiv.org/pdf/2306.07174
Mind2Web: Towards a Generalist Agent for the Web,9 Jun 2023,9 Jun 2023,https://arxiv.org/pdf/2306.06070
Can Large Language Models Infer Causation from Correlation?,9 Jun 2023,9 Jun 2023,https://arxiv.org/pdf/2306.05836
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena,9 Jun 2023,9 Jun 2023,https://arxiv.org/pdf/2306.05685
MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering,19 Dec 2022,23 May 2023,https://arxiv.org/pdf/2212.09662,ACL 2023
DePlot: One-shot visual language reasoning by plot-to-table translation,20 Dec 2022,23 May 2023,https://arxiv.org/pdf/2212.10505,ACL 2023 (Findings)
Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning,25 Jul 2022,25 Jul 2022,https://arxiv.org/pdf/2208.02294
Larger language models do in-context learning differently,7 Mar 2023,8 Mar 2023,https://arxiv.org/pdf/2303.03846
Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus,29 Sep 2022,24 Feb 2023,https://arxiv.org/pdf/2209.14927,ICLR 2023
Confident Adaptive Language Modeling,14 Jul 2022,25 Oct 2022,https://arxiv.org/pdf/2207.07061,NeurIPS 2022 Oral
Scaling Speech Technology to 1,000+ Languages,22 May 2023,22 May 2023,https://arxiv.org/pdf/2305.13516
Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models,22 May 2022,2 Nov 2022,https://arxiv.org/pdf/2205.10770,NeurIPS 2023
Luna: Linear Unified Nested Attention,3 Jun 2021,2 Nov 2021,https://arxiv.org/pdf/2106.01540,NeurIPS 2021
Scaling Laws for Reward Model Overoptimization,19 Oct 2022,19 Oct 2022,https://arxiv.org/pdf/2210.10760
Robust Speech Recognition via Large-Scale Weak Supervision,6 Dec 2022,6 Dec 2022,https://arxiv.org/pdf/2212.04356
Evolution through Large Models,17 Jun 2022,17 Jun 2022,https://arxiv.org/pdf/2206.08896
Text and Code Embeddings by Contrastive Pre-Training,24 Jan 2022,24 Jan 2022,https://arxiv.org/pdf/2201.10005
Discovering Language Model Behaviors with Model-Written Evaluations,19 Dec 2022,19 Dec 2022,https://arxiv.org/pdf/2212.09251
Orca: Progressive Learning from Complex Explanation Traces of GPT-4,5 Jun 2023,5 Jun 2023,https://arxiv.org/pdf/2306.02707
Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning,28 Mar 2023,28 Mar 2023,https://arxiv.org/pdf/2303.15647
Blockwise Parallel Transformer for Long Context Large Models,30 May 2023,30 May 2023,https://arxiv.org/pdf/2305.19370
Improving CLIP Training with Language Rewrites,31 May 2023,31 May 2023,https://arxiv.org/pdf/2305.20088
Fine-Tuning Language Models with Just Forward Passes,27 May 2023,27 May 2023,https://arxiv.org/pdf/2305.17333
Generating Images with Multimodal Language Models,6 May 2023,6 May 2023,https://arxiv.org/pdf/2305.17216
Randomized Positional Encodings Boost Length Generalization of Transformers,26 May 2023,26 May 2023,https://arxiv.org/pdf/2305.16843
A Closer Look at In-Context Learning under Distribution Shifts,26 May 2023,26 May 2023,https://arxiv.org/pdf/2305.16704
Training Socially Aligned Language Models in Simulated Human Society,26 May 2023,26 May 2023,https://arxiv.org/pdf/2305.16960
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer,25 May 2023,4 Jun 2023,https://arxiv.org/pdf/2305.16380
LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond,23 May 2023,23 May 2023,https://arxiv.org/pdf/2305.14540
Leveraging GPT-4 for Automatic Translation Post-Editing,24 May 2023,24 May 2023,https://arxiv.org/pdf/2305.14878
Is GPT-4 a Good Data Analyst?,24 May 2023,24 May 2023,https://arxiv.org/pdf/2305.15038
QLoRA: Efficient Finetuning of Quantized LLMs,23 May 2023,23 May 2023,https://arxiv.org/pdf/2305.14314
How Language Model Hallucinations Can Snowball,22 May 2023,22 May 2023,https://arxiv.org/pdf/2305.13534
Aligning Large Language Models through Synthetic Feedback,23 May 2023,23 May 2023,https://arxiv.org/pdf/2305.13735
Textually Pretrained Speech Language Models,22 May 2023,22 May 2023,https://arxiv.org/pdf/2305.13009
Augmenting Autotelic Agents with Large Language Models,21 May 2023,21 May 2023,https://arxiv.org/pdf/2305.12487
CodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Authoring,20 May 2023,20 May 2023,https://arxiv.org/pdf/2305.12050
OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models,19 May 2023,19 May 2023,https://arxiv.org/pdf/2305.12001
How Does Generative Retrieval Scale to Millions of Passages?,19 May 2023,19 May 2023,https://arxiv.org/pdf/2305.11841
PaLM 2 Technical Report,17 May 2023,17 May 2023,https://arxiv.org/pdf/2305.10403
Improved baselines for vision-language pre-training,15 May 2023,15 May 2023,https://arxiv.org/pdf/2305.08675
Interpretability at Scale: Identifying Causal Mechanisms in Alpaca,15 May 2023,15 May 2023,https://arxiv.org/pdf/2305.08809
Symbol tuning improves in-context learning in language models,15 May 2023,15 May 2023,https://arxiv.org/pdf/2305.08298
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining,17 May 2023,24 May 2023,https://arxiv.org/pdf/2305.10429
Noise2Music: Text-conditioned Music Generation with Diffusion Models,8 Feb 2023,6 Mar 2023,https://arxiv.org/pdf/2302.03917
The Flan Collection: Designing Data and Methods for Effective Instruction Tuning,31 Jan 2023,14 Feb 2023,https://arxiv.org/pdf/2301.13688
Inverse scaling can become U-shaped,3 Nov 2022,24 May 2023,https://arxiv.org/pdf/2211.02011
Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,17 Oct 2022,17 Oct 2022,https://arxiv.org/pdf/2210.09261
GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,13 Dec 2021,1 Aug 2022,https://arxiv.org/pdf/2112.06905,ICML 2022
Transformer Quality in Linear Time,21 Feb 2022,27 Jun 2022,https://arxiv.org/pdf/2202.10447,ICML 2022
Large Language Models as Tool Makers,26 May 2023,26 May 2023,https://arxiv.org/pdf/2305.17126
Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training,23 May 2023,23 May 2023,https://arxiv.org/pdf/2305.14342
Data Selection for Language Models via Importance Resampling,6 Feb 2023,6 Feb 2023,https://arxiv.org/pdf/2302.03169
Reward Design with Language Models,27 Feb 2023,27 Feb 2023,https://arxiv.org/pdf/2303.00001,ICLR 2023
Holistic Evaluation of Language Models,16 Nov 2022,16 Nov 2022,https://arxiv.org/pdf/2211.09110
Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications,7 Jun 2023,7 Jun 2023,https://arxiv.org/pdf/2306.04539
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback,22 May 2023,22 May 2023,https://arxiv.org/pdf/2305.14387
PRODIGY: Enabling In-context Learning Over Graphs,21 May 2023,21 May 2023,https://arxiv.org/pdf/2305.12600
Evaluating Verifiability in Generative Search Engines,19 Apr 2023,19 Apr 2023,https://arxiv.org/pdf/2304.09848
Learning without Forgetting for Vision-Language Models,30 May 2023,30 May 2023,https://arxiv.org/pdf/2305.19270
Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts,24 May 2023,24 May 2023,https://arxiv.org/pdf/2305.14705
CodeT5+: Open Code Large Language Models for Code Understanding and Generation,13 May 2023,20 May 2023,https://arxiv.org/pdf/2305.07922
RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs,5 May 2023,5 May 2023,https://arxiv.org/pdf/2305.08844,ACL 2023
Small Models are Valuable Plug-ins for Large Language Models,15 May 2023,15 May 2023,https://arxiv.org/pdf/2305.08848
GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content,13 May 2023,17 May 2023,https://arxiv.org/pdf/2305.07969
TESS: Text-to-Text Self-Conditioned Simplex Diffusion,15 May 2023,15 May 2023,https://arxiv.org/pdf/2305.08379
An Inverse Scaling Law for CLIP Training,11 May 2023,11 May 2023,https://arxiv.org/pdf/2305.07017
LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM,10 May 2023,10 May 2023,https://arxiv.org/pdf/2305.06404
InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,11 May 2023,11 May 2023,https://arxiv.org/pdf/2305.06500
Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction,10 May 2023,10 May 2023,https://arxiv.org/pdf/2305.06474
Multi-Task End-to-End Training Improves Conversational Recommendation,8 May 2023,8 May 2023,https://arxiv.org/pdf/2305.06218
StarCoder: may the source be with you!,9 May 2023,9 May 2023,https://arxiv.org/pdf/2305.06161
Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks,10 May 2023,10 May 2023,https://arxiv.org/pdf/2305.05862
To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review,19 Apr 2023,4 May 2023,https://arxiv.org/pdf/2304.09355
Recommender Systems with Generative Retrieval,8 May 2023,8 May 2023,https://arxiv.org/pdf/2305.05065
Towards Building the Federated GPT: Federated Instruction Tuning,9 May 2023,9 May 2023,https://arxiv.org/pdf/2305.05644
Code Execution with Pre-trained Language Models,8 May 2023,8 May 2023,https://arxiv.org/pdf/2305.05383,ACL 2023 Findings
Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens,7 May 2023,27 May 2023,https://arxiv.org/pdf/2305.04241
Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,7 May 2023,7 May 2023,https://arxiv.org/pdf/2305.04388
Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization,6 May 2023,6 May 2023,https://arxiv.org/pdf/2305.03937,ACL 2023 Findings
Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,5 May 2023,23 May 2023,https://arxiv.org/pdf/2305.03695
Masked Trajectory Models for Prediction, Representation, and Control,4 May 2023,4 May 2023,https://arxiv.org/pdf/2305.02968,ICML 2023
Data Curation Alone Can Stabilize In-context Learning,20 Dec 2022,24 May 2023,https://arxiv.org/pdf/2212.10378,ACL 2023
From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces,31 May 2023,31 May 2023,https://arxiv.org/pdf/2306.00245
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,26 Apr 2023,27 Apr 2023,https://arxiv.org/pdf/2304.13712
A Cookbook of Self-Supervised Learning,24 Apr 2023,24 Apr 2023,https://arxiv.org/pdf/2304.12210
Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback,24 Feb 2023,8 Mar 2023,https://arxiv.org/pdf/2302.12813
DocPrompting: Generating Code by Retrieving the Docs,13 Jul 2022,18 Feb 2023,https://arxiv.org/pdf/2207.05987
Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP,28 Dec 2022,23 Jan 2023,https://arxiv.org/pdf/2212.14024
Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,6 Feb 2023,12 May 2023,https://arxiv.org/pdf/2302.02662
What learning algorithm is in-context learning? Investigations with linear models,28 Nov 2022,17 May 2023,ICLR 2023
REPLUG: Retrieval-Augmented Black-Box Language Models,30 Jan 2023,24 May 2023,https://arxiv.org/pdf/2301.12652
Galactica: A Large Language Model for Science,16 Nov 2022,16 Nov 2022,https://arxiv.org/pdf/2211.09085
Atlas: Few-shot Learning with Retrieval Augmented Language Models,5 Aug 2022,16 Nov 2022,https://arxiv.org/pdf/2208.03299
What Can Transformers Learn In-Context? A Case Study of Simple Function Classes,1 Aug 2022,15 Jan 2023,https://arxiv.org/pdf/2208.01066
Exploring Length Generalization in Large Language Models,11 Jul 2022,14 Nov 2022,https://arxiv.org/pdf/2207.04901v2
Language Models (Mostly) Know What They Know,11 Jul 2022,21 Nov 2022,https://arxiv.org/pdf/2207.05221v4
Masked Autoencoders that Listen,13 Jul 2022,12 Jan 2023,https://arxiv.org/pdf/2207.06405v3,NeurIPS 2022
Solving Quantitative Reasoning Problems with Language Models,29 Jun 2022,1 Jul 2022,https://arxiv.org/pdf/2206.14858
Unveiling Transformers with LEGO: a synthetic reasoning task,9 Jun 2022,17 Feb 2023,https://arxiv.org/pdf/2206.04301
Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,9 Jun 2022,12 Jun 2023,https://arxiv.org/pdf/2206.04615
LaMDA: Language Models for Dialog Applications,20 Jan 2022,10 Feb 2022,https://arxiv.org/pdf/2201.08239
Scaling Data-Constrained Language Models,25 May 2023,30 May 2023,https://arxiv.org/pdf/2305.16264
Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations,24 May 2023,24 May 2023,https://arxiv.org/pdf/2305.14618,ACL 2023
BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,9 Nov 2022,13 Mar 2023,https://arxiv.org/pdf/2211.05100
Let's Verify Step by Step,31 May 2023,31 May 2023,https://arxiv.org/pdf/2305.20050
Scaling laws for single-agent reinforcement learning,31 Jan 2023,19 Feb 2023,https://arxiv.org/pdf/2301.13442
Direct Preference Optimization: Your Language Model is Secretly a Reward Model,29 May 2023,29 May 2023,https://arxiv.org/pdf/2305.18290
HIVE: Harnessing Human Feedback for Instructional Visual Editing,16 Mar 2023,16 Mar 2023,https://arxiv.org/pdf/2303.09618
Evaluating Large Language Models Trained on Code,7 Jul 2021,14 Jul 2021,https://arxiv.org/pdf/2107.03374
Learning Transferable Visual Models From Natural Language Supervision,26 Feb 2021,26 Feb 2021,https://arxiv.org/pdf/2103.00020
The False Promise of Imitating Proprietary LLMs,25 May 2023,25 May 2023,https://arxiv.org/pdf/2305.15717
PaLM-E: An Embodied Multimodal Language Model,6 Mar 2023,6 Mar 2023,https://arxiv.org/pdf/2303.03378
Fine-Tuning Language Models with Advantage-Induced Policy Alignment,4 Jun 2023,6 Jun 2023,https://arxiv.org/pdf/2306.02231
On Optimal Caching and Model Multiplexing for Large Model Inference,3 Jun 2023,3 Jun 2023,https://arxiv.org/pdf/2306.02003
Emergent Agentic Transformer from Chain of Hindsight Experience,26 May 2023,26 May 2023,https://arxiv.org/pdf/2305.16554,ICML 2023
Preference Transformer: Modeling Human Preferences using Transformers for RL,2 Mar 2023,2 Mar 2023,https://arxiv.org/pdf/2303.00957,ICLR 2023
Aligning Text-to-Image Models using Human Feedback,23 Feb 2023,23 Feb 2023,https://arxiv.org/pdf/2302.12192
Guiding Pretraining in Reinforcement Learning with Large Language Models,13 Feb 2023,13 Feb 2023,https://arxiv.org/pdf/2302.06692
The Wisdom of Hindsight Makes Language Models Better Instruction Followers,10 Feb 2023,10 Feb 2023,https://arxiv.org/pdf/2302.05206
Chain of Hindsight Aligns Language Models with Feedback,6 Feb 2023,25 Mar 2023,https://arxiv.org/pdf/2302.02676
Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment,2 Feb 2023,3 Feb 2023,https://arxiv.org/pdf/2302.00902
Learning Universal Policies via Text-Guided Video Generation,31 Jan 2023,2 Feb 2023,https://arxiv.org/pdf/2302.00111
MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,24 May 2023,24 May 2023,https://arxiv.org/pdf/2305.14795
DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature,26 Jan 2023,26 Jan 2023,https://arxiv.org/pdf/2301.11305
Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference,21 Nov 2022,21 Nov 2022,https://arxiv.org/pdf/2211.11875,EMNLP 2022
On Measuring the Intrinsic Few-Shot Hardness of Datasets,16 Nov 2022,16 Nov 2022,https://arxiv.org/pdf/2211.09113,EMNLP 2022
Grokking of Hierarchical Structure in Vanilla Transformers,30 May 2023,30 May 2023,https://arxiv.org/pdf/2305.18741,ACL 2023
Backpack Language Models,26 May 2023,26 May 2023,https://arxiv.org/pdf/2305.16765,ACL 2023
Meta-Learning Online Adaptation of Language Models,24 May 2023,24 May 2023,https://arxiv.org/pdf/2305.15076
Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback,24 May 2023,24 May 2023,https://arxiv.org/pdf/2305.14975
PaLI-X: On Scaling up a Multilingual Vision and Language Model,29 May 2023,29 May 2023,https://arxiv.org/pdf/2305.18565
INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models,7 Jun 2023,11 Jun 2023,https://arxiv.org/pdf/2306.04757
Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning,6 Mar 2023,6 Mar 2023,https://arxiv.org/pdf/2303.02861,ICLR 2023
Instruction Tuning with GPT-4,6 Apr 2023,6 Apr 2023,https://arxiv.org/pdf/2304.03277
Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,3 May 2023,3 May 2023,https://arxiv.org/pdf/2305.02301,ACL 2023
Locating and Editing Factual Associations in GPT,10 Feb 2022,13 Jan 2023,https://arxiv.org/pdf/2202.05262,NeurIPS 2022
Mass-Editing Memory in a Transformer,13 Oct 2022,13 Oct 2022,https://arxiv.org/pdf/2210.07229
Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval,2 Jan 2021,10 Jul 2022,https://arxiv.org/pdf/2101.00436,NeurIPS 2021 Spotlight
ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,2 Dec 2021,10 Jul 2022,https://arxiv.org/pdf/2112.01488,NAACL 2022
ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning,30 May 2023,30 May 2023,https://arxiv.org/pdf/2305.19426
Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs,23 May 2023,23 May 2023,https://arxiv.org/pdf/2305.14279
Eight Things to Know about Large Language Models,2 Apr 2023,2 Apr 2023,https://arxiv.org/pdf/2304.00612
Improving Code Generation by Training with Natural Language Feedback,28 Mar 2023,28 Mar 2023,https://arxiv.org/pdf/2303.16749
Pretraining Language Models with Human Preferences,16 Feb 2023,16 Feb 2023,https://arxiv.org/pdf/2302.08582
DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text,27 May 2023,27 May 2023,https://arxiv.org/pdf/2305.17359
PaLM: Scaling Language Modeling with Pathways,5 Apr 2022,5 Oct 2022,https://arxiv.org/pdf/2204.02311
Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,25 Mar 2021,17 Aug 2021,https://arxiv.org/pdf/2103.14030
On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?,01 March 2021,01 March 2021,https://dl.acm.org/doi/pdf/10.1145/3442188.3445922
Competition-Level Code Generation with AlphaCode,8 Feb 2022,8 Feb 2022,https://arxiv.org/pdf/2203.07814
LaMDA: Language Models for Dialog Applications,20 Jan 2022,10 Feb 2022,https://arxiv.org/pdf/2201.08239
A Path Towards Autonomous Machine Intelligence,27 Jun 2022,27 Jun 2022,https://openreview.net/pdf?id=BZ5a1r-kVsf
LoRA: Low-Rank Adaptation of Large Language Models,17 Jun 2021,16 Oct 2021,https://arxiv.org/pdf/2106.09685
GPT Understands, Too,18 Mar 2021,18 Mar 2021,https://arxiv.org/pdf/2103.10385
P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks,14 Oct 2021,20 Mar 2022,https://arxiv.org/pdf/2110.07602,ACL 2022
The Power of Scale for Parameter-Efficient Prompt Tuning,18 Apr 2021,2 Sep 2021,https://arxiv.org/pdf/2104.08691,EMNLP 2021
Prefix-Tuning: Optimizing Continuous Prompts for Generation,1 Jan 2021,1 Jan 2021,https://arxiv.org/pdf/2101.00190
On the Effectiveness of Parameter-Efficient Fine-Tuning,28 Nov 2022,28 Nov 2022,https://arxiv.org/pdf/2211.15583
Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning,11 May 2022,26 Aug 2022,https://arxiv.org/pdf/2205.05638
Editing Factual Knowledge in Language Models,16 Apr 2021,8 Sep 2021,https://arxiv.org/pdf/2104.08164,EMNLP 2022
In-Context Retrieval-Augmented Language Models,31 Jan 2023,31 Jan 2023,https://arxiv.org/pdf/2302.00083
Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,5 May 2023,5 May 2023,https://arxiv.org/pdf/2305.03268
Inspecting and Editing Knowledge Representations in Language Models,3 Apr 2023,22 May 2023,https://arxiv.org/pdf/2304.00740
How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources,7 Jun 2023,7 Jun 2023,https://arxiv.org/pdf/2306.04751
SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks,27 May 2023,27 May 2023,https://arxiv.org/pdf/2305.17390
Can Large Language Models Be an Alternative to Human Evaluations?,3 May 2023,3 May 2023,https://arxiv.org/pdf/2305.01937,ACL 2023
Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study,3 Apr 2023,10 Apr 2023,https://arxiv.org/pdf/2304.00723
Self-Refine: Iterative Refinement with Self-Feedback,30 Mar 2023,25 May 2023,https://arxiv.org/pdf/2303.17651
RARR: Researching and Revising What Language Models Say, Using Language Models,17 Oct 2022,31 May 2023,https://arxiv.org/pdf/2210.08726,ACL 2023
A Survey of Large Language Models,31 Mar 2023,7 May 2023,https://arxiv.org/pdf/2303.18223
WizardLM: Empowering Large Language Models to Follow Complex Instructions,24 Apr 2023,10 Jun 2023,https://arxiv.org/pdf/2304.12244
SELF-INSTRUCT: Aligning Language Modelswith Self-Generated Instructions,20 Dec 2022,25 May 2023,https://arxiv.org/pdf/2212.10560,ACL 2023
Training Language Models with Language Feedback at Scale,28 Mar 2023,9 Apr 2023,https://arxiv.org/pdf/2303.16755
Tree of Thoughts: Deliberate Problem Solving with Large Language Models,17 May 2023,17 May 2023,https://arxiv.org/pdf/2305.10601
A Survey on In-context Learning,31 Dec 2022,1 Jun 2023,https://arxiv.org/pdf/2301.00234
In-Context Instruction Learning,28 Feb 2023,28 Feb 2023,https://arxiv.org/pdf/2302.14691
Ask Me Anything: A simple strategy for prompting language models,5 Oct 2022,20 Nov 2022,https://arxiv.org/pdf/2210.02441
Prompting as Probing: Using Language Models for Knowledge Base Construction,23 Aug 2022,25 Aug 2022,https://arxiv.org/pdf/2208.11057