Orca: Progressive Learning from Complex
Explanation Traces of GPT-4
Subhabrata Mukherjee∗†, Arindam Mitra∗
Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah
Microsoft Research
Abstract
Recent research has focused on enhancing the capability of smaller models
through imitation learning, drawing on the outputs generated by large
foundation models (LFMs). A number of issues impact the quality of these
models, ranging from limited imitation signals from shallow LFM outputs;
small scale homogeneous training data; and most notably a lack of rigorous
evaluation resulting in overestimating the small model’s capability as they
tend to learn to imitate the style, but not the reasoning process of LFMs . To
address these challenges, we develop Orca, a 13-billion parameter model
that learns to imitate the reasoning process of LFMs. Orca learns from
rich signals from GPT-4 including explanation traces; step-by-step thought
processes; and other complex instructions, guided by teacher assistance from
ChatGPT. To promote this progressive learning, we tap into large-scale and
diverse imitation data with judicious sampling and selection. Orca surpasses
conventional state-of-the-art instruction-tuned models such as Vicuna-13B
by more than 100% in complex zero-shot reasoning benchmarks like Big-
Bench Hard (BBH) and 42%on AGIEval. Moreover, Orca reaches parity
with ChatGPT on the BBH benchmark and shows competitive performance
(4pts gap with optimized system message) in professional and academic
examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot
settings without CoT; while trailing behind GPT-4. Our research indicates
that learning from step-by-step explanations, whether these are generated
by humans or more advanced AI models, is a promising direction to improve
model capabilities and skills.
∗Co-primary authors. Author contributions listed at the end of the paper.
†Correspondence to subhabrata.mukherjee@microsoft.com
We are working with our legal team to publicly release a diff of the model weights in accordance
with LLaMA’s release policy to be published at https://aka.ms/orca-lm .
Work in progress.arXiv:2306.02707v1  [cs.CL]  5 Jun 2023

Contents
1 Introduction 4
1.1 Challenges with Existing Methods . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 Key Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Preliminaries 7
2.1 Instruction Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 Role of System Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Explanation Tuning 8
3.1 Dataset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.1.1 System Messages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.1.2 Dataset Description and Sampling from the FLAN-v2 Collection . . . 9
3.1.3 ChatGPT as Teaching Assistant . . . . . . . . . . . . . . . . . . . . . 12
3.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4 Experiment Setup 14
4.1 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.2 Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.2.1 Open-ended Generation Capabilities . . . . . . . . . . . . . . . . . . . 15
4.2.2 Reasoning Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
5 Evaluation for Open-ended Generation 17
6 Evaluation for Reasoning 17
6.1 AGIEval Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6.2 Big-Bench Hard Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
7 Evaluation for Safety 23
7.1 Truthful Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
7.2 Toxic Content Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
7.3 Note on Hallucination and Tool Augmented LFMs . . . . . . . . . . . . . . . 27
8 Limitations 28
9 Conclusions 29
10 Author Contributions 29
11 Case Studies 30
11.1 Trigonometric Problem Solving . . . . . . . . . . . . . . . . . . . . . . . . . . 30
11.2 Temporal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
11.3 Multiple-choice Question-Answering . . . . . . . . . . . . . . . . . . . . . . . 33
2

11.4 Bio Olympiad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
11.5 Forming Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
11.6 Counterfactual Question Answering . . . . . . . . . . . . . . . . . . . . . . . . 38
11.7 Compound Interest Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
11.8 Question from Vicuna-Eval . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
11.9 Spatial Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
11.10Commonsense Question Answering . . . . . . . . . . . . . . . . . . . . . . . . 42
11.11Hallucination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
11.12Quadratic Equation Solving . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
11.13Meeting Transcript Processing . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3

1 Introduction
687692 93100103
20406080100120
LLaMA-13B Alpaca-13B Vicuna-13B Bard ChatGPT Orca-13BPerformance (%) relative to 
ChatGPTEvaluation with GPT -4 
Figure 1: Orca (13B params) outperforms a wide range of foundation models including Ope-
nAI ChatGPT as evaluated by GPT-4 in the Vicuna evaluation set. We further demonstrate
similar results against a wide range of evaluation sets from other works in experiments.
3042 4247
20253035404550
Vicuna-13B Text-da-Vinci-003 Orca-13B ChatGPTAggregate  Accuracy  (%)Professional and Academic Exams (SAT, LSAT, GRE, GMAT)  (Zero -shot, MCQ)
Figure 2: Explanation tuning with Orca (13B params) bridges gap with OpenAI foundation
models like Text-da-Vinci-003 with 5 pts gap (the gap further reduces with optimized system
messages) against ChatGPT across a wide range of professional and academic exams including
GRE, GMAT, LSAT, SAT from the AGIEval benchmark [ 1] in zero-shot settings (without
any exemplar or CoT). Topical performances shown in Figure 11.
Large Foundation Models (LFMs) such as ChatGPT and GPT-4 [ 2] exhibit remarkable zero-
shot performances across a broad spectrum of tasks. Alongside academic benchmarks like
Human Eval [ 3] and Big Bench [ 4], GPT-4 has also demonstrated human-level performance
on various professional exams, including the bar exam, SAT, GRE, and USMLE. These
advancements can be credited to the scaling of both model and dataset sizes, as well
as the incorporation of a second layer of training to better align the models with user
intent. This alignment is accomplished by fine-tuning the models via supervised learning on
demonstrations of prompts and desired model behavior, and through reinforcement learning
from human preferences [5].
As these models continue to evolve and become more powerful, an intriguing question arises:
Can we use the model itself to supervise its own behavior or that of other AI models? Bai
et al.[6]have shown that by sampling output from an initial model, generating revisions,
and then fine-tuning the original model based on these revised responses, model behavior
can be controlled more effectively and can be made more harmless, with significantly fewer
human labels.
Recently, there has been an influx of studies using LFMs like ChatGPT and GPT-4 as
teachers to generate large datasets, for instruction tuning , and to train smaller models,
such as Alpaca [ 7], WizardLM [ 8] and Vicuna [ 9]. While these models can produce content
that matches the style of their teachers, they often fall short in terms of the reasoning and
comprehension skills displayed by the larger foundation models.
4

23.348.9 49.7
0102030405060
Vicuna-13B ChatGPT Orca-13BAggregate Accuracy (%)BigBench -Hard (Zero -shot, MCQ)Figure 3: For complex zero-shot reasoning tasks in BigBench-Hard, Orca achieves parity
with ChatGPT (without any exemplar or CoT) with task performances shown in Figure 12.
Take, for example, the 13-billion parameter instruction-tuned model, Vicuna [ 9] (with
LLAMA-13B [ 10] as the base), which is widely regarded as one of the best models in its
family, as evidenced by its performance on leaderboards like OpenLLM3and ChatArena4.
As illustrated in Figure 1, the widely-used evaluation method of using GPT-4 as the judge
suggests that Vicuna retains 92%of ChatGPT’s quality. However, a more meticulous
evaluation on reasoning benchmarks against human labels finds Vicuna to retain only 64%
of ChatGPT’s quality on professional and academic exams (see Figure 2), and only 48%of
ChatGPT’s quality on complex benchmarks like BigBench-hard [ 11] (see Figure 3)5. This
discrepancy not only underscores the limitations of existing evaluation protocols with smaller
LLMs, but it also reveals their significant lag in reasoning and comprehension capabilities.
In essence, these models may be articulate, but they may not necessarily possess robust
reasoning skills. In this study, we discuss some of the reasons behind these gaps and propose
strategies for addressing them.
1.1 Challenges with Existing Methods
Current research on instruction-tuning to mimic the output of LFM’s like ChatGPT exhibits
notable limitation in task diversity, query complexity, and data scaling. These observations
are corroborated in a recent study by Gudibande et al. [12], where the authors assert that
“model imitation is a false promise” since “broadly matching ChatGPT using purely imitation
would require (1) a concerted effort to collect enormous imitation datasets and (2) far more
diverse and higher quality imitation data than is currently available.”. Contrary to this
assertion, we demonstrate that both conditions (1) and (2) are attainable and that it is
possible to reduce the gap with proprietary LLM’s on multiple zero-shot benchmarks that
require sophisticated reasoning. We elaborate on these challenges below:
Simple instructions with limited diversity. The Self-Instruct [ 13] process involves using
an initial set of prompts to incite the LFM to produce new instructions. Any low-quality or
overly similar responses are then removed, and the remaining instructions are reintegrated
into the task pool for further iterations. Nonetheless, the resulting queries generated through
Self-Instruct, such as “what are the three primary colors?", “what is the capital of France?", etc. ,
can exhibit limitations in diversity and complexity. Both Alpaca [ 7] and WizardLM [ 8]
employ a variant of self-instruct. WizardLM introduces the concept of Evol-Instruct, which
gradually rewrites the initial set of instructions into more complex versions, attempting to
overcome some of the method’s inherent shortcomings. On the other hand, recent works
like Vicuna [ 9] and Koala [ 14] demonstrate remarkable performance due to more human-like
conversations and natural instructions in community-contributed conversations like those in
ShareGPT6that provided a forum for users to share their conversations with ChatGPT.
Task diversity and data scaling. Human-contributed conversations in ShareGPT are a
valuable source of data, but they also have some limitations. They tend to favor creative
3https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
4https://chat.lmsys.org/?arena
5ChatGPT may have data contamination issues with respect to BigBench
6https://sharegpt.com/
5

content generation and information-seeking queries over other types of tasks. Therefore,
models trained on such natural conversations may capture the style but not the reasoning
process of the LFMs – demonstrated in the performance of Vicuna in Figures 2 and 3.
Additionally, such mode of data collection is also limited in scale. Table 1 shows an overview
of the size of data and tuning methods employed in recent popular instruction tuning works.
Limited imitation signals. Existing methods rely on immitation learning from
⟨query, response⟩pairs generated by the teacher model. However, this provides limited
signals to trace the reasoning process of the teacher. Prior works [ 15,16] on open-box model
show that richer signals such as logits, intermediate representations and attention states can
significantly improve distillation performance. While they are not accessible for closed-box
LFM’s7, recent work [ 17] demonstrates that richer signals like LFM rationales can help close
the gap for task-specific distillation.
Evaluation: Previous studies on instruction tuning of small models with LFMs are severely
limited in their evaluation protocol. They often rely on GPT-4 for auto-evaluation by asking
it to compare the outputs of two systems with a prompt like “given responses from system
1 (reference) and system 2 (target), which one is better?”. However, this approach has
several drawbacks, such as the small size of test sets (e.g., 80instructions in Vicuna and 218
instructions in WizardLM) and the biases of GPT-4 as the judge [ 18]. For example, we notice
that models that are instruction-tuned with GPT-4 responses tend to generate longer texts
that GPT-4 prefers over shorter ones; as well as GPT-4 has a bias in the order of the candidate
responses. We will show that such auto-evaluation measures overestimate the abilities of
smaller models compared to LFMs, as the former are much weaker in comprehension and
reasoning skills.
1.2 Key Contributions
In this research, our focus is on addressing the challenges mentioned above, specifically with:
Explanation tuning: We augment⟨query, response⟩pairs with detailed responses from
GPT-4 that explain the reasoning process of the teacher as it generates the response. These
provide the student with additional signals for learning. We leverage system instructions (e.g..,
explain like I’m five, think step-by-step and justify your response , etc.) to
elicit such explanations. This is in contrast to vanilla instruction tuning, which only uses the
prompt and the LFM response for learning, providing little opportunity for mimicking the
LFM’s “thought” process.
Scaling tasks and instructions: We utilize the Flan 2022 Collection [ 19] as it provides
an extensive public assortment of tasks and instructions. Particularly, we use FLAN-
v2, supplemented with high-quality templates, advanced formatting patterns, and data
augmentations. Even though FLAN holds tens of millions of instructions, we selectively
sample from the task collection to form a diverse mixture of tasks, which we then further
sub-sample to generate complex prompts. These prompts are used to query LFMs like
ChatGPT and GPT-4, thus creating a rich and diverse training set. We collect 5million
ChatGPT responses, from which 1million is further sampled to acquire GPT-4 responses.
We demonstrate how ChatGPT as a teacher assistant helps in progressive learning.
Evaluation: We assess the generative, reasoning, and comprehension abilities of Orca, under
a range of settings: (i) AutoEvaluation with GPT-4 on existing evaluation sets from Vicuna,
WizardLM and the awesome prompts collection8; (ii) Academic benchmarks like Big-Bench
Hard [4] and TruthfulQA [ 20]; (iii) Professional and Academic exams like SAT, LSAT, GRE,
GMAT from AGIEval [ 1]; (iv) Safety evaluation with ToxiGen [ 21] to test toxic language
generation and hate speech detection across different minority groups. Finally, we provide
case-studies to compare the generation and reasoning abilities of Orca against OpenAI LFMs
like ChatGPT and GPT-4, and instruction-tuned smaller model like Vicuna.
7Note that OpenAI API’s do give access to the top-5logits for each token.
8https://prompts.chat/
6