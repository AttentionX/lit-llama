Published as a conference paper at ICLR 2023
REAC T: S YNERGIZING REASONING AND ACTING IN
LANGUAGE MODELS
Shunyu Yao*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2
1Department of Computer Science, Princeton University
2Google Research, Brain team
1{shunyuy,karthikn}@princeton.edu
2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com
ABSTRACT
While large language models (LLMs) have demonstrated impressive performance
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action
plan generation) have primarily been studied as separate topics. In this paper, we
explore the use of LLMs to generate both reasoning traces and task-speciﬁc actions
in an interleaved manner, allowing for greater synergy between the two: reasoning
traces help the model induce, track, and update action plans as well as handle
exceptions, while actions allow it to interface with and gather additional information
from external sources such as knowledge bases or environments. We apply our
approach, named ReAct , to a diverse set of language and decision making tasks
and demonstrate its effectiveness over state-of-the-art baselines in addition to
improved human interpretability and trustworthiness. Concretely, on question
answering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent
issues of hallucination and error propagation in chain-of-thought reasoning by
interacting with a simple Wikipedia API, and generating human-like task-solving
trajectories that are more interpretable than baselines without reasoning traces.
Furthermore, on two interactive decision making benchmarks (ALFWorld and
WebShop), ReAct outperforms imitation and reinforcement learning methods by
an absolute success rate of 34% and 10% respectively, while being prompted with
only one or two in-context examples.
1 I NTRODUCTION
A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with
verbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to
play an important role in human cognition for enabling self-regulation or strategization (Vygotsky,
1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-
sider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may
reason in language in order to track progress (“now that everything is cut, I should heat up the pot of
water”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let
me use soy sauce and pepper instead”), and to realize when external information is needed (“how do
I prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the
recipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What
dish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans
to learn new tasks quickly and perform robust decision making or reasoning, even under previously
unseen circumstances or facing information uncertainties.
Recent results have hinted at the possibility of combining verbal reasoning with interactive decision
making in autonomous systems. On one hand, properly prompted large language models (LLMs)
have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive
Work during Google internship. Projet page with code: https://react-lm.github.io/ .
1arXiv:2210.03629v3  [cs.CL]  10 Mar 2023

Published as a conference paper at ICLR 2023
$FW7KLQN>)LUVW,QHHGWRILQGDSHSSHUVKDNHUPRUHOLNHO\WRDSSHDULQFDELQHWVFRXQWHUWRSV@$FW*RWRFDELQHW2EV2QWKHFDELQHW\RXVHHDYDVH+HUHWKHDJHQWJRWRFDELQHWWKHQFDELQHWWKHQFRXQWHUWRSDQG$FW*RWRFRXQWHUWRS2EV2QWKHFRXQWHUWRS\RXVHHDDSSOHDEUHDGDSHSSHUVKDNHUDQGDYDVH$FW7DNHSHSSHUVKDNHUIURPFRXQWHUWRS2EV<RXSLFNXSWKHSHSSHUVKDNHUIURPWKHFRXQWHUWRS$FW7KLQN>1RZ,ILQGDSHSSHUVKDNHU1H[W,QHHGWRSXWLWLQRQGUDZHU@$FW*RWRGUDZHU2EV'UDZHULVFORVHG$FW2SHQGUDZHU2EV<RXRSHQ'UDZHUª$FW3XWSHSSHUVKDNHULQRQGUDZHU2EV<RXSXWSHSSHUVKDNHULQRQWKHGUDZHUD6WDQGDUG$QVZHUL3RGE&R75HDVRQ2QO\7KRXJKW/HW
VWKLQNVWHSE\VWHS$SSOH5HPRWHZDVRULJLQDOO\GHVLJQHGWRLQWHUDFWZLWK$SSOH79$SSOH79FDQEHFRQWUROOHGE\L3KRQHL3DGDQGL3RG7RXFK6RWKHDQVZHULVL3KRQHL3DGDQGL3RG7RXFK$QVZHUL3KRQHL3DGL3RG7RXFKF$FW2QO\$FW6HDUFK>$SSOH5HPRWH@2EV7KH$SSOH5HPRWHLVDUHPRWHFRQWUROª$FW6HDUFK>)URQW5RZ@2EV&RXOGQRWILQG>)URQW5RZ@6LPLODUª$FW6HDUFK>)URQW5RZVRIWZDUH@2EV)URQW5RZLVDGLVFRQWLQXHGPHGLDFHQWHUVRIWZDUHª$FW)LQLVK>\HV@G5H$FW5HDVRQ$FW7KRXJKW,QHHGWRVHDUFK$SSOH5HPRWHDQGILQGWKHSURJUDPLWZDVRULJLQDOO\GHVLJQHGWRLQWHUDFWZLWK$FW6HDUFK>$SSOH5HPRWH@2EV7KH$SSOH5HPRWHLVDUHPRWHFRQWUROLQWURGXFHGLQ2FWREHUE\$SSOHªRULJLQDOO\GHVLJQHGWRFRQWUROWKH)URQW5RZPHGLDFHQWHUSURJUDPª7KRXJKW$SSOH5HPRWHZDVRULJLQDOO\GHVLJQHGWRFRQWUROWKH)URQW5RZPHGLDFHQWHUSURJUDP,QHHGWRVHDUFK)URQW5RZQH[WDQGILQGZKDWRWKHUGHYLFHFDQFRQWUROLW$FW6HDUFK>)URQW5RZ@2EV&RXOGQRWILQG>)URQW5RZ@6LPLODU>
)URQW5RZ6HDWWR(DUWK

)URQW5RZ0RWRUVSRUWV

)URQW5RZVRIWZDUH
ª@7KRXJKW)URQW5RZLVQRWIRXQG,QHHGWRVHDUFK)URQW5RZVRIWZDUH$FW6HDUFK>)URQW5RZVRIWZDUH@2EV)URQW5RZLVDGLVFRQWLQXHGPHGLDFHQWHUVRIWZDUHª7KRXJKW)URQW5RZVRIWZDUHLVFRQWUROOHGE\DQ$SSOH5HPRWHRUWKHNH\ERDUGIXQFWLRQNH\V6RWKHDQVZHULVNH\ERDUGIXQFWLRQNH\V$FW)LQLVK>NH\ERDUGIXQFWLRQNH\V@Ҽ4XHVWLRQ$VLGHIURPWKH$SSOH5HPRWHZKDWRWKHUGHYLFHFDQFRQWUROWKHSURJUDP$SSOH5HPRWHZDVRULJLQDOO\GHVLJQHGWRLQWHUDFWZLWK"
D$FW2QO\$FW*RWRGUDZHU2EV7KHGUDZHULVFORVHG$FW2SHQGUDZHU2EV<RXRSHQWKHGUDZHU7KHGUDZHULVRSHQ,QLW\RXVHHDGLVKVSRQJHDQGDVSRRQ$FW*RWRVLQNEDVLQ2EV2QWKHVLQNEDVLQ\RXVHHDGLVKVSRQJHDVSDWXODDQGDVSRRQ$FW7DNHSHSSHUVKDNHUIURPVLQNEDVLQ2EV1RWKLQJKDSSHQV$FW7DNHSHSSHUVKDNHUIURPVLQNEDVLQ2EV1RWKLQJKDSSHQVE5H$FW5HDVRQ$FW
Ҽ<RXDUHLQWKHPLGGOHRIDURRP/RRNLQJTXLFNO\DURXQG\RX\RXVHHDFDELQHWDFDELQHWDFRIIHHPDFKLQHDFRXQWHUWRSDVWRYHEXUQHUDQGDWRDVWHU<RXUWDVNLVWR3XWVRPHSHSSHUVKDNHURQDGUDZHU$OI:RUOG+RWVSRW4$
Figure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) Chain-of-thought ( CoT,
Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)
question. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar
et al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task
solving trajectories generated by the model (Act, Thought) and the environment (Obs).
answers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,
2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses
its own internal representations to generate thoughts and is not grounded in the external world,
which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact
hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,
recent work has explored the use of pre-trained language models for planning and acting in interactive
environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with
a focus on predicting actions via language priors. These approaches usually convert multi-modal
observations into text, use a language model to generate domain-speciﬁc actions or plans, and then
use a controller to choose or execute them. However, they do not employ language models to reason
abstractly about high-level goals or maintain a working memory to support acting, barring Huang
et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the
current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been
studies on how reasoning and acting can be combined in a synergistic manner for general task solving,
and if such a combination can bring systematic beneﬁts compared to reasoning or acting alone.
In this work, we present ReAct , a general paradigm to combine reasoning and acting with language
models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct
prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an
interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and
adjust high-level plans for acting (reason to act), while also interact with the external environments
(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).
2

Published as a conference paper at ICLR 2023
We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:
question answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),
text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao
et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact
with, ReAct outperforms vanilla action generation models while being competitive with chain-of-
thought reasoning ( CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct
andCoT that allows for the use of both internal knowledge and externally obtained information
during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able
to outperform imitation or reinforcement learning methods trained with 103105task instances,
with an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate
the importance of sparse, versatile reasoning in decision making by showing consistent advantages
over controlled baselines with actions only. Besides general applicability and performance boost,
the combination of reasoning and acting also contributes to model interpretability, trustworthiness,
and diagnosability across all domains, as humans can readily distinguish information from model’s
internal knowledge versus external environments, as well as inspect reasoning traces to understand
the decision basis of model actions.
To summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt-
based paradigm to synergize reasoning and acting in language models for general task solving; (2) we
perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a
few-shot learning setup over prior approaches that perform either reasoning or action generation in
isolation; (3) we present systematic ablations and analysis to understand the importance of acting in
reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the
prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ﬁnetuning
experiments showing the potential of ReAct to improve with additional training data. Scaling up
ReAct to train and operate on more tasks and combining it with complementary paradigms like
reinforcement learning could further unlock the potential of large language models.
2REAC T: SYNERGIZING REASONING +AC TING
Consider a general setup of an agent interacting with an environment for task solving. At time
stept, an agent receives an observation ot2O from the environment and takes an action at2A
following some policy (atjct), wherect= (o1;a1;;ot 1;at 1;ot)is the context to the agent.
Learning a policy is challenging when the mapping ct7!atis highly implicit and requires extensive
computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct ﬁnal
action (Act 4) to ﬁnish the QA task as it requires complex reasoning over the trajectory context
(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the
context that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.
The idea of ReAct is simple: we augment the agent’s action space to ^A=A[L , whereLis the
space of language. An action ^at2L in the language space, which we will refer to as a thought or a
reasoning trace , does not affect the external environment, thus leading to no observation feedback.
Instead, a thought ^ataims to compose useful information by reasoning over the current context ct,
and update the context ct+1= (ct;^at)to support future reasoning or acting. As shown in Figure 1,
there could be various types of useful thoughts, e.g. decomposing task goals and create action plans
(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),
extracting important parts from observations (1d, Thought2, 4), track progress and transit action plans
(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.
However, as the language space Lis unlimited, learning in this augmented action space is difﬁcult
and requires strong language priors. In this paper, we mainly focus on the setup where a frozen
large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context
examples to generate both domain-speciﬁc actions and free-form language thoughts for task solving
(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and
environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is
of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the
task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision
making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to
1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.
3

Published as a conference paper at ICLR 2023
appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the
asynchronous occurrence of thoughts and actions for itself.
Since decision making and reasoning capabilities are integrated into a large language model, ReAct
enjoys several unique features: A) Intuitive and easy to design : Designing ReAct prompts is
straightforward as human annotators just type down their thoughts in language on top of their actions
taken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail
prompt design for each task in Sections 3 and 4. B) General and ﬂexible : Due to the ﬂexible thought
space and thought-action occurrence format, ReAct works for diverse tasks with distinct action
spaces and reasoning needs, including but not limited to QA, fact veriﬁcation, text game, and web
navigation. C) Performant and robust :ReAct shows strong generalization to new task instances
while learning solely from one to six in-context examples, consistently outperforming baselines with
only reasoning or acting across different domains. We also show in Section 3 additional beneﬁts
when ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.
D) Human aligned and controllable :ReAct promises an interpretable sequential decision making
and reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,
humans can also control or correct the agent behavior on the go by thought editing, as shown in
Figure 5 in Section 4.
3 K NOWLEDGE -INTENSIVE REASONING TASKS
We begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact
veriﬁcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to
retrieve information to support reasoning, while also use reasoning to target what to retrieve next,
demonstrating a synergy of reasoning and acting.
3.1 S ETUP
Domains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-
PotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning
over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation
benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based
on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only
setup for both tasks, where models only receive the question/claim as input without access to support
paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with
an external environment to support reasoning.
Action Space We design a simple Wikipedia web API with three types of actions to support
interactive information retrieval: (1) search [entity ], which returns the ﬁrst 5 sentences from
the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the
Wikipedia search engine, (2) lookup [string ], which would return the next sentence in the page
containing string , simulating Ctrl+F functionality on the browser. (3) finish [answer ], which
would ﬁnish the current task with answer . We note that this action space mostly can only retrieve a
small part of a passage based on exact passage name, which is signiﬁcantly weaker than state-of-the-
art lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,
and force models to retrieve via explicit reasoning in language.
3.2 M ETHODS
ReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2from the training
set and manually compose ReAct -format trajectories to use as few-shot exemplars in the prompts.
Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense
thought), where free-form thoughts are used for various purposes. Speciﬁcally, we use a combination
of thoughts that decompose questions (“I need to search x, ﬁnd y, then ﬁnd z”), extract information
from Wikipedia observations (“x was started in 1844”, “The paragraph does not tell x”), perform
commonsense (“x is not y, so z must instead be...”) or arithmetic reasoning (“1844 < 1989”), guide
2We ﬁnd more examples do not improve performance.
4

Published as a conference paper at ICLR 2023
Prompt Methoda HotpotQA Fever
(EM) (Acc)
Standard 28.7 57.1
CoT (Wei et al., 2022) 29.4 56.3
CoT-SC (Wang et al., 2022a) 33.4 60.4
Act 25.7 58.9
ReAct 27.4 60.9
CoT-SC!ReAct 34.2 64.6
ReAct!CoT-SC 35.1 62.0
Supervised SoTAb67.5 89.5
Table 1: PaLM-540B prompting results on
HotpotQA and Fever.
aHotpotQA EM is 27.1, 28.9, 33.8 for Standard ,CoT,
CoT-SC in Wang et al. (2022b).
b(Zhu et al., 2021; Lewis et al., 2020)
0 5 10 15 20
#CoT-SC trials2628303234HotpotQA EM
0 5 10 15 20
#CoT-SC trials47.550.052.555.057.560.062.565.0Fever AccMethod
CoT-SC -> ReAct
ReAct -> CoT-SC
CoT-SC
ReAct
CoTFigure 2: PaLM-540B prompting results with respect to
number of CoT-SC samples used.
search reformulation (“maybe I can search/look up x instead”), and synthesize the ﬁnal answer (“...so
the answer is x”). See Appendix C for more details.
Baselines We systematically ablate ReAct trajectories to build prompts for multiple baselines (with
formats as Figure 1(1a-1c)): (a) Standard prompting (Standard ), which removes all thoughts,
actions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,
2022), which removes actions and observations and serve as a reasoning-only baseline. We also
build a self-consistency baseline ( CoT-SC ) (Wang et al., 2022a;b) by sampling 21 CoT trajectories
with decoding temperature 0.7 during inference and adopting the majority answer, which is found to
consistently boost performance over CoT. (c)Acting-only prompt (Act), which removes thoughts
inReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the
Internet to answer questions, though it operates on a different task and action space, and uses imitation
and reinforcement learning instead of prompting.
Combining Internal and External Knowledge As will be detail in Section 3.3, we observe that
the problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT
is more accurate in formulating reasoning structure but can easily suffer from hallucinated facts
or thoughts. We therefore propose to incorporate ReAct andCoT-SC , and let the model decide
when to switch to the other method based on the following heuristics: A) ReAct!CoT-SC : when
ReAct fails to return an answer within given steps, back off to CoT-SC . We set 7 and 5 steps for
HotpotQA and FEVER respectively as we ﬁnd more steps will not improve ReAct performance3.
B)CoT-SC!ReAct : when the majority answer among nCoT-SC samples occurs less than n=2
times (i.e. internal knowledge might not support the task conﬁdently), back off to ReAct .
Finetuning Due to the challenge of manually annotating reasoning traces and actions at scale,
we consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories
with correct answers generated by ReAct (also for other baselines) to ﬁnetune smaller language
models (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on
input questions/claims. More details are in Appendix B.1.
3.3 R ESULTS AND OBSERVATIONS
ReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-
540B as the base model with different prompting methods. We note that ReAct is better than Act
on both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the
ﬁnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conﬁrm the beneﬁt of reasoning
traces for more informed acting.
3Of all trajectories with correct ﬁnal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only
take up 0.84% and 1.33% respectively.
5

Published as a conference paper at ICLR 2023
Type Deﬁnition ReAct CoT
SuccessTrue positive Correct reasoning trace and facts 94% 86%
False positive Hallucinated reasoning trace or facts 6% 14%
FailureReasoning error Wrong reasoning trace (including failing to recover from repetitive steps) 47% 16%
Search result error Search return empty or does not contain useful information 23% -
Hallucination Hallucinated reasoning trace or facts 0% 56%
Label ambiguity Right prediction but did not match the label precisely 29% 28%
Table 2: Types of success and failure modes of ReAct andCoT on HotpotQA, as well as their
percentages in randomly selected examples studied by human.
ReAct vs.CoT On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly
lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only
differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge
is vital. To better understand the behavioral difference between ReAct andCoT on HotpotQA, we
randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct
andCoT respectively (thus 200 examples in total), and manually labeled their success and failure
modes in Table 2. Some key observations are as follows:
A)Hallucination is a serious problem for CoT, resulting in much higher false positive rate than
ReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the
problem solving trajectory of ReAct is more grounded, fact-driven, and trustworthy, thanks to the
access of an external knowledge base.
B)While interleaving reasoning, action and observation steps improves ReAct ’s grounded-
ness and trustworthiness, such a structural constraint also reduces its ﬂexibility in formulating
reasoning steps , leading to more reasoning error rate than CoT. we note that there is one frequent
error pattern speciﬁc to ReAct , in which the model repetitively generates the previous thoughts and
actions, and we categorize it as part of “reasoning error” as the model fails to reason about what the
proper next action to take and jump out of the loop4.
C)ForReAct , successfully retrieving informative knowledge via search is critical. Non-
informative search, which counts for 23% of the error cases, derails the model reasoning and gives
it a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between
factuality and ﬂexibility, which motivates our proposed strategies of combining two methods.
We provide examples for each success and failure modes in Appendix E.1. We also ﬁnd some
HotpotQA questions may contain outdated answer labels, see Figure 4 for example.
ReAct +CoT-SC perform best for prompting LLMs Also shown in Table 1, the best prompting
method on HotpotQA and Fever are ReAct!CoT-SC andCoT-SC!ReAct respectively.
Furthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC
samples used. While two ReAct +CoT-SC methods are advantageous at one task each, they both
signiﬁcantly and consistently outperform CoT-SC across different number of samples, reaching
CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of
properly combining model internal knowledge and external knowledge for reasoning tasks.
ReAct performs best for ﬁne-tuning Figure 3 shows the scaling effect of prompting/ﬁnetuning
four methods ( Standard ,CoT,Act,ReAct ) on HotpotQA. With PaLM-8/62B, prompting ReAct
performs worst among four methods due to the difﬁculty to learn both reasoning and acting from
in-context examples. However, when ﬁnetuned with just 3,000 examples, ReAct becomes the best
method among the four, with PaLM-8B ﬁnetuned ReAct outperforming all PaLM-62B prompting
methods, and PaLM-62B ﬁnetuned ReAct outperforming all 540B prompting methods. In contrast,
ﬁnetuning Standard orCoT is signiﬁcantly worse than ﬁnetuning ReAct orAct for both PaLM-
8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge
facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a
more generalizable skill for knowledge reasoning. As all prompting methods are still signiﬁcantly
far from domain-speciﬁc state-of-the-art approaches (Table 1), we believe ﬁnetuning with more
human-written data might be a better way to unleash the power of ReAct .
4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using
better decoding (e.g. beam search) might help address this issue.
6