Direct Preference Optimization:
Your Language Model is Secretly a Reward Model
Rafael Rafailov∗†Archit Sharma∗†Eric Mitchell∗†
Stefano Ermon†‡Christopher D. Manning†Chelsea Finn†
†Stanford University‡CZ Biohub
{rafailov,architsh,eric.mitchell}@cs.stanford.edu
Abstract
While large-scale unsupervised language models (LMs) learn broad world knowl-
edge and some reasoning skills, achieving precise control of their behavior is
difficult due to the completely unsupervised nature of their training. Existing
methods for gaining such steerability collect human labels of the relative quality of
model generations and fine-tune the unsupervised LM to align with these prefer-
ences, often with reinforcement learning from human feedback (RLHF). However,
RLHF is a complex and often unstable procedure, first fitting a reward model that
reflects the human preferences, and then fine-tuning the large unsupervised LM
using reinforcement learning to maximize this estimated reward without drifting
too far from the original model. In this paper, we leverage a mapping between
reward functions and optimal policies to show that this constrained reward maxi-
mization problem can be optimized exactly with a single stage of policy training,
essentially solving a classification problem on the human preference data. The
resulting algorithm, which we call Direct Preference Optimization (DPO), is stable,
performant, and computationally lightweight, eliminating the need for fitting a
reward model, sampling from the LM during fine-tuning, or performing significant
hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to
align with human preferences as well as or better than existing methods. Notably,
fine-tuning with DPO exceeds RLHF’s ability to control sentiment of generations
and improves response quality in summarization and single-turn dialogue while
being substantially simpler to implement and train.
1 Introduction
Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabili-
ties [ 11,7,37,8]. However, these models are trained on data generated by humans with a wide variety
of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for
example, while we may want our AI coding assistant to understand common programming mistakes
in order to correct them, nevertheless, when generating code, we would like to bias our model toward
the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want
our language model to be aware of a common misconception believed by 50% of people, but we
certainly do not want the model to claim this misconception to be true in 50% of queries about it!
In other words, selecting the model’s desired responses and behavior from its very wide knowledge
and abilities is crucial to building AI systems that are safe, performant, and controllable [ 23]. While
existing methods typically steer LMs to match human preferences using reinforcement learning (RL),
∗Equal contribution; more junior authors listed earlier.
Preprint. Under review.arXiv:2305.18290v1  [cs.LG]  29 May 2023

Figure 1: DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods
for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and
human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.
In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification
objective, without an explicit reward function or RL.
we will show that the RL-based objective used by existing methods can be optimized exactly with a
simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.
At a high level, existing methods instill the desired behaviors into a language model using curated
sets of human preferences representing the types of behaviors that humans find safe and helpful. This
preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on
a large text dataset. While the most straightforward approach to preference learning is supervised
fine-tuning on human demonstrations of high quality responses, the most successful class of methods
is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [ 12,2]). RLHF methods fit
a reward model to a dataset of human preferences and then use RL to optimize a language model
policy to produce responses assigned high reward without drifting excessively far from the original
model. While RLHF produces models with impressive conversational and coding abilities, the RLHF
pipeline is considerably more complex than supervised learning, involving training multiple LMs and
sampling from the LM policy in the loop of training, incurring significant computational costs.
In this paper, we show how to directly optimize a language model to adhere to human preferences,
without explicit reward modeling or reinforcement learning. We propose Direct Preference Optimiza-
tion (DPO) , an algorithm that implicitly optimizes the same objective as existing RLHF algorithms
(reward maximization with a KL-divergence constraint) but is simple to implement and straight-
forward to train. Intuitively, the DPO update increases the relative log probability of preferred to
dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents
the model degeneration that we find occurs with a naive probability ratio objective. Like existing
algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; [ 5]) that
measures how well a given reward function aligns with empirical preference data. However, while
existing methods use the preference model to define a preference loss to train a reward model and
then train a policy that optimizes the learned reward model, DPO uses a change of variables to define
the preference loss as a function of the policy directly. Given a dataset of human preferences over
model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective,
without explicitly learning a reward function or sampling from the policy during training.
Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for
training language models from preferences. Our experiments show that DPO is at least as effective
as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as
sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.
2 Related Work
Self-supervised language models of increasing scale learn to complete some tasks zero-shot [ 28] or
with few-shot prompts [ 6,22,11]. However, their performance on downstream tasks and alignment
with user intent can be significantly improved by fine-tuning on datasets of instructions and human-
written completions [ 21,33,13,36]. This ‘instruction-tuning’ procedure enables LLMs to generalize
to instructions outside of the instruction-tuning set and generally increase their usability [ 13]. Despite
the success of instruction tuning, relative human judgments of response quality are often easier to
collect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of
human preferences, improving proficiency in translation [ 16], summarization [ 35,45], story-telling
[45], and instruction-following [ 23,29]. These methods first optimize a neural network reward
function for compatibility with the dataset of preferences under a preference model such as the
Bradley-Terry model [ 5], then fine-tune a language model to maximize the given reward using
2

reinforcement learning algorithms, commonly REINFORCE [ 41], proximal policy optimization
(PPO; [ 34]), or variants [ 29]. A closely-related line of work leverages LLMs fine-tuned for instruction
following with human feedback to generate additional synthetic preference data for targeted attributes
such as safety or harmlessness [ 2], using only weak supervision from humans in the form of a
text rubric for the LLM’s annotations. These methods represent a convergence of two bodies of
work: one body of work on training language models with reinforcement learning for a variety
of objectives [ 30,24,42] and another body of work on general methods for learning from human
preferences [ 12,17]. Despite the appeal of using relative human preferences, fine-tuning large
language models with reinforcement learning remains a major practical challenge; this work provides
a theoretically-justified approach to optimizing relative preferences without RL.
Outside of the context of language, learning policies from preferences has been studied in both bandit
and reinforcement learning settings, and several approaches have been proposed. Contextual bandit
learning using preferences or rankings of actions, rather than rewards, is known as a contextual
dueling bandit (CDB; [ 44,14]). In the absence of absolute rewards, theoretical analysis of CDBs
substitutes the notion of an optimal policy with a von Neumann winner , a policy whose expected win
rate against anyother policy is at least 50% [ 14]. However, in the CDB setting, preference labels
are given online, while in learning from human preferences, we typically learn from a fixed batch of
offline preference-annotated action pairs [ 43]. Similarly, preference-based RL (PbRL) learns from
binary preferences generated by an unknown ‘scoring’ function rather than rewards [9, 32]. Various
algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally
involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently
optimizing it [ 15,9,12,31,17]. We instead present a single stage policy learning approach that
directly optimizes a policy to satisfy preferences.
3 Preliminaries
We review the RLHF pipeline in Ziegler et al., which has also been adopted in subsequent work
[35,1,23]. It usually consists of three phases: 1) supervised fine-tuning (SFT); 2) preference
sampling and reward learning and 3) reinforcement-learning optimization.
SFT phase : RLHF typically begins with a generic pre-trained LM, which is fine-tuned with supervised
learning (maximum likelihood) on a high-quality dataset for the downstream task(s) of interest, such
as dialogue, instruction following, summarization, etc., to obtain a model πSFT.
Reward Modelling Phase : In the second phase the SFT model is prompted with prompts xto
produce pairs of answers (y1, y2)∼πSFT(y|x). These are then presented to human labelers
who express preferences for one answer, denoted as yw≻yl|xwhere ywandyldenotes the
preferred and dispreferred completion amongst (y1, y2)respectively. The preferences are assumed
to be generated by some latent reward model r∗(y, x), which we do not have access to. There are a
number of approaches used to model preferences, the Bradley-Terry (BT) [ 5] model being a popular
choice (although more general Plackett-Luce ranking models [ 27,19] are also compatible with the
framework if we have access to several ranked answers). The BT model stipulates that the human
preference distribution p∗can be written as:
p∗(y1≻y2|x) =exp (r∗(x, y 1))
exp (r∗(x, y 1)) + exp ( r∗(x, y 2)). (1)
Assuming access to a static dataset of comparisons D=
x(i), y(i)
w, y(i)
l	N
i=1sampled from p∗, we
can parametrize a reward model rϕ(x, y)and estimate the parameters via maximum likelihood.
Framing the problem as a binary classification we have the negative log-likelihood loss:
LR(rϕ,D) =−E(x,yw,yl)∼D
logσ(rϕ(x, yw)−rϕ(x, yl))
(2)
where σis the logistic function. In the context of LMs, the network rϕ(x, y)is often initialized from
the SFT model πSFT(y|x)with the addition of a linear layer on top of the final transformer layer
that produces a single scalar prediction for the reward value [ 45]. To ensure a reward function with
lower variance, prior works normalize the rewards, such that Ex,y∼D[rϕ(x, y)] = 0 for all x.
RL Fine-Tuning Phase : During the RL phase, we use the learned reward function to provide
feedback to the language model. In particular, we formulate the following optimization problem
max
πθEx∼D,y∼πθ(y|x)
rϕ(x, y)
−βDKL
πθ(y|x)||πref(y|x)
(3)
3

where βis a parameter controlling the deviation from the base reference policy πref, namely the ini-
tial SFT model πSFT. In practice, the language model policy πθis also initialized to πSFT. The
added constraint is important, as it prevents the model from deviating too far from the distri-
bution on which the reward model is accurate, as well as maintaining the generation diversity
and preventing mode-collapse to single high-reward answers. Due to the discrete nature of lan-
guage generation, this objective is not differentiable and is typically optimized with reinforce-
ment learning. The standard approach [ 45,35,1,23] has been to construct the reward function
r(x, y) =rϕ(x, y)−β(logπθ(y|x)−logπref(y|x)), and maximize using PPO [34].
4 Direct Preference Optimization
Motivated by the challenges of applying reinforcement learning algorithms on large-scale problems
such as fine-tuning language models, our goal is to derive a simple approach for policy optimization
using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it
via RL, our approach bypasses the reward modeling step and directly optimizes a language model
using preference data. As we will describe next in detail, our key insight is to leverage an analytical
mapping from reward functions to optimal policies, which enables us to transform a loss function
over reward functions into a loss function over policies. This change-of-variables approach allows
us to skip the explicit reward modeling step, while still optimizing under existing models of human
preferences, such as the Bradley-Terry model. In essence, the policy network represents both the
language model and the reward.
Deriving the DPO objective. We start with the same RL objective as prior work, Eq. 3, under
a general reward function r. Following prior work [ 26,25], it is straightforward to show that the
optimal solution to the KL-constrained reward maximization objective in Eq. 3 takes the form:
πr(y|x) =1
Z(x)πref(y|x) exp1
βr(x, y)
, (4)
where Z(x) =P
yπref(y|x) exp
1
βr(x, y)
is the partition function. See Appendix A.1 for a
complete derivation. Even if we use the MLE estimate rϕof the ground-truth reward function r∗,
it is still difficult to estimate the partition function Z(x), which makes this representation hard to
utilize in practice. However, we can rearrange Eq. 4 to express the reward function in terms of its
corresponding optimal policy πr, the reference policy πref, and the unknown partition function Z(·).
Specifically, we first take the logarithm of both sides of Eq. 4 and then with some algebra we obtain:
r(x, y) =βlogπr(y|x)
πref(y|x)+βlogZ(x). (5)
We can apply this reparameterization to the ground-truth reward r∗and corresponding optimal model
π∗. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two
completions, i.e., p∗(y1≻y2|x) =σ(r∗(x, y 1)−r∗(x, y 2)). Substituting the reparameterization
in Eq. 5 for r∗(x, y)into the preference model Eq. 1, the partition function cancels, and we can
express the human preference probability in terms of only the optimal policy π∗and reference policy
πref. Thus, the optimal RLHF policy π∗under the Bradley-Terry model satisfies the preference model:
p∗(y1≻y2|x) =1
1 + exp
βlogπ∗(y2|x)
πref(y2|x)−βlogπ∗(y1|x)
πref(y1|x) (6)
The derivation is in Appendix A.2. While Eq. 6 uses the Bradley-Terry model, we can similarly
derive expressions under the more general Plackett-Luce models [27, 19], shown in Appendix A.3.
Now that we have the probability of human preference data in terms of the optimal policy rather than
the reward model, we can formulate a maximum likelihood objective for a parametrized policy πθ.
Analogous to the reward modeling approach (i.e. Eq. 2), our policy objective becomes:
LDPO(πθ;πref) =−E(x,yw,yl)∼D
logσ
βlogπθ(yw|x)
πref(yw|x)−βlogπθ(yl|x)
πref(yl|x)
. (7)
This way, we simultaneously bypass the explicit reward modeling step while also avoiding the need
to perform reinforcement learning optimization. Moreover, since our procedure is equivalent to fitting
a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies
4

under suitable assumption of the preference data distribution [ 4]. In Section 5, we further discuss
theoretical properties of DPO in relation to other works.
What does the DPO update do? For a mechanistic understanding of DPO, it is useful to analyze the
gradient of the loss function LDPO. The gradient with respect to the parameters θcan be written as:
∇θLDPO(πθ;πref) =
−βE(x,yw,yl)∼D
σ(ˆrθ(x, yl)−ˆrθ(x, yw))| {z }
higher weight when reward estimate is wrong
∇θlogπ(yw|x)| {z }
increase likelihood of yw− ∇ θlogπ(yl|x)|{z }
decrease likelihood of yl
,
where ˆrθ(x, y) =βlogπθ(y|x)
πref(y|x)is the reward implicitly defined by the language model πθand refer-
ence model πref(more in Section 5). Intuitively, the gradient of the loss function LDPOincreases the
likelihood of the preferred completions ywand decreases the likelihood of dispreferred completions
yl. Importantly, the examples are weighed by how much higher the implicit reward model ˆrθrates
the dispreferred completions, scaled by β, i.e, how incorrectly the implicit reward model orders
the completions, accounting for the strength of the KL constraint. Our experiments suggest the
importance of this weighting, as a naïve version of this method without the weighting coefficient can
cause the language model to degenerate (Appendix Table 2).
DPO outline. The general DPO pipeline is as follows: 1) Sample completions y1, y2∼πref(· |x)
for every prompt x, label with human preferences to construct the offline dataset of preferences
D={x(i), y(i)
w, yl)(i)}N
i=1and 2) optimize the language model πθto minimize LDPOfor the given
πrefandDand desired β. In practice, one would like to reuse preference datasets publicly available,
rather than generating samples and gathering human preferences. Since the preference datasets
are sampled using πSFT, we initialize πref=πSFTwhenever available. However, when πSFTis
not available, we initialize πrefby maximizing likelihood of preferred completions (x, yw), that
is,πref= arg maxπEx,yw∼D[logπ(yw|x)]. This procedure helps mitigate the distribution shift
between the true reference distribution which is unavailable, and πrefused by DPO. Further details
related to the implementation and hyperparameters can be found in Appendix B.
5 Theoretical Analysis of DPO
In this section, we give further interpretation of the DPO method, provide theoretical backing, and
relate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO [34]).
5.1 Your Language Model Is Secretly a Reward Model
DPO is able to bypass both explicit reward estimation and RL to learn the policy using a single
maximum likelihood objective. However, the optimization objective Eq. 5 is equivalent to a Bradley-
Terry model with a reward function r∗(x, y) =βlogπ∗
θ(y|x)
πref(y|x)and we optimize our parametric model
πθ, equivalently to the reward model optimization in Eq. 2 under the this change of variables. In this
section we will build the theory behind this reparameterization, show that it does not constrain the
class of learned reward models, and allows for the exact recovery of the optimal policy. We begin
with by defining an equivalence relation between reward functions.
Definition 1. We say that two reward functions r(x, y)and r′(x, y)are equivalent iff
r(x, y)−r′(x, y) =f(x)for some function f.
It is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions
into classes. We can state the following two lemmas:
Lemma 1. Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two
reward functions from the same class induce the same preference distribution.
Lemma 2. Two reward functions from the same equivalence class induce the same optimal policy
under the constrained RL problem.
The proofs are straightforward and we defer them to Appendix A.5. The first lemma is a well-known
under-specification issue with the Plackett-Luce family of models [ 27]. Due to this under-specification,
we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE
estimates from Eq. 2 [ 4]. The second lemma states that all reward functions from the same class
yield the same optimal policy, hence for our final objective, we are only interested in recovering an
arbitrary reward function from the optimal class. We prove the following Theorem in Appendix A.6:
5

Theorem 1. Under mild assumptions, all reward classes consistent with the Plackett-Luce
(and Bradley-Terry in particular) models can be represented with the reparameterization
r(x, y) =βlogπ(y|x)
πref(y|x)for some model π(y|x)and a given reference model πref(y|x).
Proof Sketch. Consider any reward function r(x, y), which induces a corresponding optimal model
πr(y|x), specified by Eq. 4. We will show that a reward function from the equivalence class of r
can be represented using the reparameterization given above. We define the projection fas
f(r;πref, β)(x, y) =r(x, y)−βlogX
yπref(y|x) exp1
βr(x, y)
(8)
The operator fsimply normalizes the reward function with the logarithm of the partition function
ofπr. Since the added normalization term is only a function of the prefix x,f(r;πref, β)(x, y)is a
reward function in the equivalence class of r(x, y). Finally, replacing rwith the RHS of Eq. 5 (which
holds for any reward function), we have f(r;πref, β)(x, y) =βlogπr(y|x)
πref(y|x). That is, the projection
fproduces a member of the equivalence class of rwith the desired form, and we do not lose any
generality in our reward model from the proposed reparameterization.
We can alternatively view Theorem 1 as specifying exactly which reward function within each
equivalence class the DPO reparameterization selects, that is, the reward function satisfying:
X
yπref(y|x) exp1
βr(x, y)
| {z }
=π(y|x), using Thm. 1 reparam.= 1, (9)
i.e.,π(y|x)is a valid distribution (probabilities are positive and sum to 1). However, following
Eq. 4, we can see that Eq. 9 is the partition function of the optimal policy induced by the reward
function r(x, y). The key insight of the DPO algorithm is that we can impose certain constraints on
the under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models,
such that we preserve the class of representable reward models, but explicitly make the optimal policy
in Eq. 4 analytically tractable for all prompts x.
5.2 Instability of Actor-Critic Algorithms
We can also use our framework to diagnose instabilities with standard actor-critic algorithms used
for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step
outlined in Section 3. We can draw connections to the control as inference framework [ 18] for the
constrained RL problem outlined in 3. We assume a parameterized model πθ(y|x)and minimize
DKL[πθ(y|x)||π∗(y|x)]where π∗is the optimal policy from Eq. 7 induced by the reward function
rϕ(y, x). With some algebra this leads to the optimization objective:
max
πθEπθ(y|x)
rϕ(x, y)−βlogX
yπrefexp1
βrϕ(x, y)
| {z }
f(rϕ,πref,β)−βlogπθ(y|x)
πref(y|x)|{z }
KL
(10)
This is the same objective optimized in prior works [ 45,35,1,23] using the DPO-equivalent reward
for the reward class of rϕ. In this setting, we can interpret the normalization term in f(rϕ, πref, β)
as the soft value function of the reference policy πref. While this term does not affect the optimal
solution, without it, the policy gradient of the objective could have high variance, making learning
unstable. We can accommodate for the normalization term using a learned value function, but that
can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human
completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In
contrast the DPO reparameterization yields a reward function that does not require any baselines.
6 Experiments
In this section, we empirically evaluate DPO’s ability to train policies directly from preferences. First,
in a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing
reward and minimizing KL-divergence with the reference policy, compared to common preference
6