QL ORA: Efficient Finetuning of Quantized LLMs
Tim Dettmers∗Artidoro Pagnoni∗Ari Holtzman
Luke Zettlemoyer
University of Washington
{dettmers,artidoro,ahai,lsz}@cs.washington.edu
Abstract
We present QLORA, an efficient finetuning approach that reduces memory us-
age enough to finetune a 65B parameter model on a single 48GB GPU while
preserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-
ents through a frozen, 4-bit quantized pretrained language model into Low Rank
Adapters (LoRA). Our best model family, which we name Guanaco , outperforms
all previous openly released models on the Vicuna benchmark, reaching 99.3%
of the performance level of ChatGPT while only requiring 24 hours of finetuning
on a single GPU. QLORAintroduces a number of innovations to save memory
without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that
is information theoretically optimal for normally distributed weights (b) Double
Quantization to reduce the average memory footprint by quantizing the quantization
constants, and (c) Paged Optimizers to manage memory spikes. We use QLORA
to finetune more than 1,000 models, providing a detailed analysis of instruction
following and chatbot performance across 8 instruction datasets, multiple model
types (LLaMA, T5), and model scales that would be infeasible to run with regular
finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA
finetuning on a small high-quality dataset leads to state-of-the-art results, even
when using smaller models than the previous SoTA. We provide a detailed analysis
of chatbot performance based on both human and GPT-4 evaluations showing that
GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-
thermore, we find that current chatbot benchmarks are not trustworthy to accurately
evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates
where Guanaco fails compared to ChatGPT. We release all of our models and code,
including CUDA kernels for 4-bit training.2
1 Introduction
Finetuning large language models (LLMs) is a highly effective way to improve their performance,
[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,
finetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B
parameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization
methods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for
inference and break down during training [65].
We demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any
performance degradation. Our method, QLORA, uses a novel high-precision technique to quantize
a pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]
∗Equal contribution.
2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes
Preprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023

Table 1: Elo ratings for a competition between
models, averaged for 10,000 random initial order-
ings. The winner of a match is determined by
GPT-4 which declares which response is better for
a given prompt of the the Vicuna benchmark. 95%
confidence intervals are shown ( ±). After GPT-
4, Guanaco 33B and 65B win the most matches,
while Guanaco 13B scores better than Bard.
Model Size Elo
GPT-4 - 1348 ±1
Guanaco 65B 41 GB 1022 ±1
Guanaco 33B 21 GB 992 ±1
Vicuna 13B 26 GB 974 ±1
ChatGPT - 966 ±1
Guanaco 13B 10 GB 916 ±1
Bard - 902 ±1
Guanaco 7B 6 GB 879 ±1that are tuned by backpropagating gradients through
the quantized weights.
QLORAreduces the average memory requirements
of finetuning a 65B parameter model from >780GB
of GPU memory to <48GB without degrading the
runtime or predictive performance compared to a 16-
bit fully finetuned baseline. This marks a significant
shift in accessibility of LLM finetuning: now the
largest publicly available models to date finetunable
on a single GPU. Using QLORA, we train the Gua-
naco family of models, with the second best model
reaching 97.8% of the performance level of ChatGPT
on the Vicuna [ 10] benchmark, while being trainable
in less than 12 hours on a single consumer GPU;
using a single professional GPU over 24 hours we
achieve 99.3% with our largest model, essentially
closing the gap to ChatGPT on the Vicuna bench-
mark. When deployed, our smallest Guanaco model
(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than
20 percentage points on the Vicuna benchmark (Table 6).
QLORAintroduces multiple innovations designed to reduce memory use without sacrificing per-
formance: (1) 4-bit NormalFloat , an information theoretically optimal quantization data type for
normally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.
(2)Double Quantization , a method that quantizes the quantization constants, saving an average
of about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers ,
using NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when
processing a mini-batch with a long sequence length. We combine these contributions into a better
tuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of
the accuracy tradeoffs seen in prior work.
QLORA’s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot
performance on model scales that would be impossible using regular finetuning due to memory
overhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,
model architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA
recovers 16-bit performance (§4) and training a state-of-the-art chatbot, Guanaco , (§5), we also
analyze trends in the trained models. First, we find that data quality is far more important than
dataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,
subsampled) on chatbot performance, even when both are meant to support instruction following
generalization. Second, we show that strong Massive Multitask Language Understanding (MMLU)
benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice
versa—in other words, dataset suitability matters more than size for a given task.
Furthermore, we also provide a extensive analysis of chatbot performance that uses both human
raters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete
against each other in matches to produce the best response for a given prompt. The winner of a
match is judged by either GPT-4 or human annotators. The tournament results are aggregated into
Elo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and
human evaluations largely agree on the rank of model performance in the tournaments, but we also
find there are instances of strong disagreement. As such, we highlight that model-based evaluation
while providing a cheap alternative to human-annotation also has its uncertainties.
We augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-
sis highlights success and failure cases that were not captured by the quantitative benchmarks.
We release all model generations with human and GPT-4 annotations to facilitate further study. We
open-source our codebase and CUDA kernels and integrate our methods into the Hugging Face
transformers stack [ 64], making them easily accessible to all. We release a collection of adapters
for 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32
different open sourced, finetuned models.
2

Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by
quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.
2 Background
Block-wise k-bit Quantization Quantization is the process of discretizing an input from a rep-
resentation that holds more information to a representation with less information. It often means
taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to
8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is
commonly rescaled into the target data type range through normalization by the absolute maximum
of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit
Floating Point (FP32) tensor into a Int8 tensor with range [−127,127]:
XInt8=round127
absmax (XFP32)XFP32
=round (cFP32·XFP32), (1)
where cis the quantization constant orquantization scale . Dequantization is the inverse:
dequant (cFP32,XInt8) =XInt8
cFP32=XFP32(2)
The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input
tensor, then the quantization bins—certain bit combinations—are not utilized well with few or no
numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the
input tensor into blocks that are independently quantized, each with their own quantization constant c.
This can be formalized as follows: We chunk the input tensor X∈Rb×hintoncontiguous blocks of
sizeBby flattening the input tensor and slicing the linear segment into n= (b×h)/Bblocks. We
quantize these blocks independently with Equation 1 to create a quantized tensor and nquantization
constants ci.
Low-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory
requirements by using a small set of trainable parameters, often termed adapters, while not updating
the full model parameters which remain fixed. Gradients during stochastic gradient descent are
passed through the fixed pretrained model weights to the adapter, which is updated to optimize the
loss function. LoRA augments a linear projection through an additional factorized projection. Given
a projection XW =YwithX∈Rb×h,W∈Rh×oLoRA computes:
Y=XW +sXL 1L2, (3)
whereL1∈Rh×randL2∈Rr×o, and sis a scalar.
Memory Requirement of Parameter-Efficient Finetuning One important point of discussion is
the memory requirement of LoRA during training both in terms of the number and size of adapters
used. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve
performance without significantly increasing the total memory used. While LoRA was designed as a
3

Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning
comes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA
model trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used
0.2% of the original model weights[ 28,37], the LoRA input gradients have a memory footprint
of 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [ 9], the
input gradients reduce to an average of 18 MB per sequence making them more memory intensive
than all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of
memory. This highlights that gradient checkpointing is important but also that aggressively reducing
the amount of LoRA parameter yields only minor memory benefits. This means we can use more
adapters without significantly increasing the overall training memory footprint (see Appendix G
for a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision
performance.
3 QL ORA Finetuning
QLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat
(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to
prevent memory spikes during gradient checkpointing from causing out-of-memory errors that have
traditionally made finetuning on a single machine difficult for large models.
QLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data
type that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we
dequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.
We now discuss the components of QL ORA followed by a formal definition of QL ORA.
4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization
[15] which is an information-theoretically optimal data type that ensures each quantization bin has an
equal number of values assigned from the input tensor. Quantile quantization works by estimating
the quantile of the input tensor through the empirical cumulative distribution function.
The main limitation of quantile quantization is that the process of quantile estimation is expensive.
Therefore fast quantile approximation algorithms, such as SRAM quantiles [ 15], are used to estimate
them. Due to the approximate nature of these quantile estimation algorithms, the data type has large
quantization errors for outliers, which are often the most important values.
Expensive quantile estimates and approximation errors can be avoided when input tensors come from
a distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles
making exact quantile estimation computationally feasible.
Since pretrained neural network weights usually have a zero-centered normal distribution with
standard deviation σ(see Appendix F), we can transform all weights to a single fixed distribution by
scaling σsuch that the distribution fits exactly into the range of our data type. For our data type, we
set the arbitrary range [−1,1]. As such, both the quantiles for the data type and the neural network
weights need to be normalized into this range.
The information theoretically optimal data type for zero-mean normal distributions with arbitrary
standard deviations σin the range [−1,1]is computed as follows: (1) estimate the 2k+ 1quantiles
of a theoretical N(0,1)distribution to obtain a k-bit quantile quantization data type for normal distri-
butions, (2) take this data type and normalize its values into the [−1,1]range, (3) quantize an input
weight tensor by normalizing it into the [−1,1]range through absolute maximum rescaling.
Once the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to
rescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data
type. More formally, we estimate the 2kvalues qiof the data type as follows:
qi=1
2
QXi
2k+ 1
+QXi+ 1
2k+ 1
, (4)
where QX(·)is the quantile function of the standard normal distribution N(0,1). A problem for
a symmetric k-bit quantization is that this approach does not have an exact representation of zero,
which is an important property to quantize padding and other zero-valued elements with no error. To
4

ensure a discrete zeropoint of 0and to use all 2kbits for a k-bit datatype, we create an asymmetric
data type by estimating the quantiles qiof two ranges qi:2k−1for the negative part and 2k−1+ 1for
the positive part and then we unify these sets of qiand remove one of the two zeros that occurs in both
sets. We term the resulting data type that has equal expected number of values in each quantization bin
k-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered
normally distributed data. The exact values of this data type can be found in Appendix E.
Double Quantization We introduce Double Quantization (DQ), the process of quantizing the
quantization constants for additional memory savings. While a small blocksize is required for precise
4-bit quantization [ 13], it also has a considerable memory overhead. For example, using 32-bit
constants and a blocksize of 64 for W, quantization constants add 32/64 = 0 .5bits per parameter on
average. Double Quantization helps reduce the memory footprint of quantization constants.
More specifically, Double Quantization treats quantization constants cFP32
2of the first quantization
as inputs to a second quantization. This second step yields the quantized quantization constants
cFP8
2and the second level of quantization constants cFP32
1. We use 8-bit Floats with a blocksize of
256 for the second quantization as no performance degradation is observed for 8-bit quantization,
in line with results from Dettmers and Zettlemoyer [13]. Since the cFP32
2are positive, we subtract
the mean from c2before quantization to center the values around zero and make use of symmetric
quantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per
parameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits
per parameter.
Paged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page
transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU
occasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM
and the disk. We use this feature to allocate paged memory for the optimizer states which are then
automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU
memory when the memory is needed in the optimizer update step.
QL ORA.Using the components described above, we define QLORAfor a single linear layer in
the quantized base model with a single LoRA adapter as follows:
YBF16=XBF16doubleDequant (cFP32
1, ck-bit
2,WNF4) +XBF16LBF16
1LBF16
2, (5)
where doubleDequant (·)is defined as:
doubleDequant (cFP32
1, ck-bit
2,Wk-bit) =dequant (dequant (cFP32
1, ck-bit
2),W4bit) =WBF16,(6)
We use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision
and a blocksize of 256 for c2to conserve memory.
For parameter updates only the gradient with respect to the error for the adapters weights∂E
∂Liare
needed, and not for 4-bit weights∂E
∂W. However, the calculation of∂E
∂Lientails the calculation of∂X
∂W
which proceeds via equation (5) with dequantization from storage WNF4to computation data type
WBF16to calculate the derivative∂X
∂Win BFloat16 precision.
To summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation
data type (16-bit BrainFloat). We dequantize the storage data type to the computation data type
to perform the forward and backward pass, but we only compute weight gradients for the LoRA
parameters which use 16-bit BrainFloat.
4 QLoRA vs. Standard Finetuning
We have discussed how QLoRA works and how it can significantly reduce the required memory for
finetuning models. The main question now is whether QLoRA can perform as well as full-model
finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of
NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed
at answering these questions.
3https://docs.nvidia.com/cuda/cuda-c-programming-guide
5

Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only)
and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our
evaluations include GLUE [ 58] with RoBERTa-large [ 38], Super-NaturalInstructions (TKInstruct)
[61] with T5 [ 49], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [ 39] and Alpaca
[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of
Dettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity
across different models (OPT [ 72], LLaMA [ 57], BLOOM [ 52], Pythia [ 7]) for model sizes 125m -
13B. We provide more details in the results section for each particular setup to make the results more
readable. Full details in Appendix A.
QLoRA-AllQLoRA-FFN
QLoRA-AttentionAlpaca (ours)
Stanford-Alpaca
Model6061626364RougeL
bits
4
16
Figure 2: RougeL for LLaMA 7B models on the
Alpaca dataset. Each point represents a run with a
different random seed. We improve on the Stanford
Alpaca fully finetuned default hyperparameters to
construct a strong 16-bit baseline for comparisons.
Using LoRA on all transformer layers is critical to
match 16-bit performance.While paged optimizers are critical to do 33B/65B
QLORAtuning on a single 24/48GB GPU, we do
not provide hard measurements for Paged Optimiz-
ers since the paging only occurs when processing
mini-batches with long sequence lengths, which is
rare. We do, however, perform an analysis of the
runtime of paged optimizers for 65B models on
48GB GPUs and find that with a batch size of 16,
paged optimizers provide the same training speed
as regular optimizers. Future work should measure
and characterize under what circumstances slow-
downs occur from the paging process.
Default LoRA hyperparameters do not match 16-
bit performance When using the standard prac-
tice of applying LoRA to query and value attention
projection matrices [ 28], we are not able to replicate
full finetuning performance for large base models.
As shown in Figure 2 for LLaMA 7B finetuning on
Alpaca, we find that the most critical LoRA hyper-
parameter is how many LoRA adapters are used in
total and that LoRA on all linear transformer block
layers are required to match full finetuning perfor-
mance. Other LoRA hyperparameters, such as the
projection dimension r, do not affect performance (see Appendix A).
1010
1011
T otal model bits
0.60
0.61
0.62
0.63
0.64
0.65
0.66
0.67Mean zeroshot accuracy
4-bit LLaMA
Float
NFloat
NFloat + DQData type
Figure 3: Mean zero-shot accuracy over Wino-
grande, HellaSwag, PiQA, Arc-Easy, and Arc-
Challenge using LLaMA models with different 4-bit
data types. The NormalFloat data type significantly
improves the bit-for-bit accuracy gains compared
to regular 4-bit Floats. While Double Quantization
(DQ) only leads to minor gains, it allows for a more
fine-grained control over the memory footprint to fit
models of certain size (33B/65B) into certain GPUs
(24/48GB).Similarly, we find that default hyperparameters for
fully finetuned baselines are undertuned. We do a
hyperparameter search over learning rates 1e-6 to
5e-5 and batch sizes 8 to 128 to find robust baselines.
Results for 7B LLaMA finetuning on Alpaca are
shown in Figure 2.
4-bit NormalFloat yields better performance
than 4-bit Floating Point While the 4-bit
NormalFloat (NF4) data type is information-
theoretically optimal, it still needs to be determined
if this property translates to empirical advantages.
We follow the setup from Dettmers and Zettlemoyer
[13] where quantized LLMs (OPT [ 72], BLOOM
[52], Pythia [ 7], LLaMA) of different sizes (125M
to 65B) with different data types are evaluated on
language modeling and a set of zero-shot tasks. In
Figure 3 and Table 2 we see that NF4 improves per-
formance significantly over FP4 and Int4 and that
double quantization reduces the memory footprint
without degrading performance.
k-bit QL ORAmatches 16-bit full finetuning and
16-bit LoRA performance Recent findings have
established that 4-bit quantization for inference is
6