import requests
import io
import PyPDF2
import numpy as np

import sys
from pathlib import Path

# support running without installing as a package
wd = Path(__file__).parent.parent.resolve()
sys.path.append(str(wd))

TOKENIZER_PATH = '/Users/adamlee/Downloads/AttentionX/models/llama/tokenizer/tokenizer.model'
destination_path = Path('../data/peft')

urls = [
    'https://arxiv.org/pdf/2303.08774.pdf',
]

# Prepare dataset for arxiv articles
def parseArxiv(text, destination_path: Path = Path("data/alpaca")):
    """Prepare the "Tiny Shakespeare" dataset."""
    from lit_llama import Tokenizer
    print(f"Preparing the arxiv dataset ... {len(text)} characters")
    title = text.split('\n')[0]

    # Tokenizer.train(input=input_file_path, destination=destination_path, vocab_size=100)
    tokenizer = Tokenizer(TOKENIZER_PATH)
    text_ids = tokenizer.encode(text)
    print(f"{title} has {len(text_ids):,} tokens")
    text_ids = np.array(text_ids, dtype=np.uint16)
    text_ids.tofile(destination_path / f"{title}.bin")

def retrieveArxiv(url):
    # Download the PDF file from the URL
    response = requests.get(url)

    # Extract the content of the PDF file as bytes
    content = io.BytesIO(response.content)

    # Read the PDF file using PyPDF2
    pdf_reader = PyPDF2.PdfReader(content)

    # Extract the text from the PDF file
    text = ''
    for page_num in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_num]
        text += page.extract_text()
        if page_num == 10:
            break

    # Print the extracted text
    print(text)
    parseArxiv(text, destination_path)

if __name__ == '__main__':
    for url in urls:
        retrieveArxiv(url)